[
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "projects.html#repo-for-all-my-projects",
    "href": "projects.html#repo-for-all-my-projects",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "Projects/project4.html",
    "href": "Projects/project4.html",
    "title": "Client Report - [Prediction of Houses Built Before 1980 in Colorado]",
    "section": "",
    "text": "Show the code\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree \nfrom sklearn.metrics import RocCurveDisplay\n\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import uniform, randint\n\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\nfrom scipy.stats import uniform, randint\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import accuracy_score\nShow the code\nrng = np.random.RandomState(31337)",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#elevator-pitch",
    "href": "Projects/project4.html#elevator-pitch",
    "title": "Client Report - [Prediction of Houses Built Before 1980 in Colorado]",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nLooking at Random Forest, Decision Tree and XGboost the highest accuracy rate was xgboost with over 96% accurracy using a joined dwellings and neighborhood data frame. There was a significant increase in accuracy for all models when using the joined dataframes.\n::: {#cell-project data .cell execution_count=4}\n\nRead and format project data\n# Include and execute your code here\n\ndwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')\ndf = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')\n\ndf2 = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_neighborhoods_ml/dwellings_neighborhoods_ml.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nparcel\nabstrprd\nlivearea\nfinbsmnt\nbasement\nyrbuilt\ntotunits\nstories\nnocars\nnumbdrm\n...\narcstyle_THREE-STORY\narcstyle_TRI-LEVEL\narcstyle_TRI-LEVEL WITH BASEMENT\narcstyle_TWO AND HALF-STORY\narcstyle_TWO-STORY\nqualified_Q\nqualified_U\nstatus_I\nstatus_V\nbefore1980\n\n\n\n\n0\n00102-08-065-065\n1130\n1346\n0\n0\n2004\n1\n2\n2\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n1\n00102-08-073-073\n1130\n1249\n0\n0\n2005\n1\n1\n1\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n2\n00102-08-078-078\n1130\n1346\n0\n0\n2005\n1\n2\n1\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n3\n00102-08-081-081\n1130\n1146\n0\n0\n2005\n1\n1\n0\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n4\n00102-08-086-086\n1130\n1249\n0\n0\n2005\n1\n1\n1\n2\n...\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n\n\n\n\n5 rows × 51 columns\n\n\n:::\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-1",
    "href": "Projects/project4.html#questiontask-1",
    "title": "Client Report - [Prediction of Houses Built Before 1980 in Colorado]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\nThere are many factors that we can use to help our algorithms learn and predict homes that were made before 1980. It is important to not flood the algorithm with unneeded data. So I spent time testing different numbers of variables used in the algorithms.\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .34, random_state = 66)\n\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\n    #70, .30, 66, .34, 80,.20\n    #     x='parcel',\n#     y='abstrpod',\n#     color='yrbuilt',\n#     title='Does age affect whether a passenger survived?',\n#     # marginal=\"box\", # or violin, rug\n#     # hover_data=\n# )\n# clf = xgb_model #Rand Forest is similar, try it. \n#linear regression,\n# clf = RandomForestClassifier \n# clf = clf.fit(X_train, y_train)\n# # Fit: fit the model to said data\n# y_pred = clf.predict(X_test)\n# y_probs = clf.predict_proba(X_test)\n# y_pred = clf.predict(X_test)\n\n\nLooking at the chart we see that almost 25% was from one-story buildings. After this the effectivness of the potential relationships drop drastically. So figuring out which data is almost irrelevant, in this case removing data that is well below 1% helpts my algorithms learn and predict houses made before 1980.\n\n\nShow the code\n# # clf is a classifier.\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n# #linear regression, \n# clf = RandomForestClassifier()\nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n\n# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)\n\n\n\n\nShow the code\ndf_features = pd.DataFrame(\n    {'f_names': X_train.columns, \n    'f_values': clf.feature_importances_}).sort_values('f_values', ascending = False)\n\n\n\n\nShow the code\n#%%\nchart = px.bar(df_features.head(20), #You can change this 10 based on top features you want to see.\n    x='f_values', \n    y='f_names'\n)\n\nchart.update_layout(yaxis={'categoryorder':'total ascending'})\n# 0-1 is based on % 1 is 100% \n# f(x) = y (y is target, does not change here)\n# f is algorithm, Try 4\n# (x) is features",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-2",
    "href": "Projects/project4.html#questiontask-2",
    "title": "Client Report - [Prediction of Houses Built Before 1980 in Colorado]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\nBelow I will show you some different results based on test size and 3 different Algorithms. These algorithms are RandomForest, Decision Tree and XGboost. XGboost got the highest accuracy at over 96% when dwellings and neighborhood data were joined.\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .34, random_state = 66)\n\n    # Extract the top 10 feature names\n\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\n# Assuming df_features is already created and sorted by feature importance\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = RandomForestClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = RandomForestClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"RandomForest Accuracy with Top 20 Features 34/66:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.91      0.90      2834\n           1       0.95      0.93      0.94      4957\n\n    accuracy                           0.92      7791\n   macro avg       0.92      0.92      0.92      7791\nweighted avg       0.92      0.92      0.92      7791\n\nRandomForest Accuracy with Top 20 Features 34/66: 0.9273520729046335\n\n\n\n\nShow the code\n# Assuming df_features is already created and sorted by feature importance\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = tree.DecisionTreeClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"DecisionTreeClassifier Accuracy with Top 20 Features 34/66:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.87      0.87      0.87      2907\n           1       0.92      0.92      0.92      4884\n\n    accuracy                           0.90      7791\n   macro avg       0.89      0.89      0.89      7791\nweighted avg       0.90      0.90      0.90      7791\n\nDecisionTreeClassifier Accuracy with Top 20 Features 34/66: 0.9007829546913105\n\n\n\n\nShow the code\n# Correct the syntax for dropping columns and selecting the target variable\nX = dwellings_ml.drop(['before1980', 'yrbuilt', 'parcel'], axis=1) # Assuming you want to drop these columns\ny = dwellings_ml['before1980'] # Correct way to select the target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=66)\n\n# Initialize the XGBClassifier model\nxgb_model = XGBClassifier(objective='binary:logistic', n_estimators=500, learning_rate=0.34, random_state=66)\n\n# Fit the model\nxgb_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = xgb_model.predict(X_test)\n\n# Evaluation\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"This matrix shows the true positive, false positive, true negative, and false negative predictions.\")\nprint(\"Confusion Matrix:\") \nprint(conf_matrix)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'XGboost Accuracy 34/66: {accuracy}')\n\n\nThis matrix shows the true positive, false positive, true negative, and false negative predictions.\nConfusion Matrix:\n[[2593  312]\n [ 275 4611]]\nXGboost Accuracy 34/66: 0.9246566551148762\n\n\n\n\nShow the code\nX = dwellings_ml.drop(['before1980', 'yrbuilt', 'parcel'], axis=1) # Assuming you want to drop these columns\ny = dwellings_ml['before1980'] # Correct way to select the target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=64)\n\n# Initialize the XGBClassifier model\nxgb_model = XGBClassifier(objective='binary:logistic', n_estimators=500, learning_rate=0.3, random_state=43)\n\n# Fit the model\nxgb_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = xgb_model.predict(X_test)\n\n# Evaluation\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"This matrix shows the true positive, false positive, true negative, and false negative predictions.\")\nprint(\"Confusion Matrix:\") \nprint(conf_matrix)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'XGboost Accuracy 34/66: {accuracy}')\n\n\nThis matrix shows the true positive, false positive, true negative, and false negative predictions.\nConfusion Matrix:\n[[2616  306]\n [ 254 4615]]\nXGboost Accuracy 34/66: 0.9281221922731356\n\n\nThis next set of data shows how changing the test from 34|66 to 30|70 affected the accuracy. In most cases the accuracy dropped however, I noticed there were times where the Random Forest actually went slightly higher. \n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .30, random_state = 70)\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\n# Assuming df_features is already created and sorted by feature importance\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = RandomForestClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = RandomForestClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"RandomForest Accuracy with Top 20 Features 30/70:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.90      0.89      2574\n           1       0.94      0.93      0.94      4300\n\n    accuracy                           0.92      6874\n   macro avg       0.91      0.92      0.92      6874\nweighted avg       0.92      0.92      0.92      6874\n\nRandomForest Accuracy with Top 20 Features 30/70: 0.9268257201047425\n\n\n\n\nShow the code\n# Assuming df_features is already created and sorted by feature importance\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = tree.DecisionTreeClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"DecisionTreeClassifier Accuracy with Top 20 Features 30/70:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.87      0.86      0.87      2636\n           1       0.92      0.92      0.92      4238\n\n    accuracy                           0.90      6874\n   macro avg       0.89      0.89      0.89      6874\nweighted avg       0.90      0.90      0.90      6874\n\nDecisionTreeClassifier Accuracy with Top 20 Features 30/70: 0.897585103287751\n\n\n\n\nShow the code\n# Correct the syntax for dropping columns and selecting the target variable\nX = dwellings_ml.drop(['before1980', 'yrbuilt', 'parcel'], axis=1) # Assuming you want to drop these columns\ny = dwellings_ml['before1980'] # Correct way to select the target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=70)\n\n# Initialize the XGBClassifier model\nxgb_model = XGBClassifier(objective='binary:logistic', n_estimators=500, learning_rate=0.3, random_state=43)\n\n# Fit the model\nxgb_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = xgb_model.predict(X_test)\n\n# Evaluation\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(\"Confusion Matrix:\") \nprint(conf_matrix)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'XGboost Accuracy: {accuracy}')\n\n\nConfusion Matrix:\n[[2345  259]\n [ 241 4029]]\nXGboost Accuracy: 0.927262147221414\n\n\nFor this set I am using the 80/20 test and with this the accuracy went back up. The highest was XGboost reaching 93.1% accuracy. \n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .2, random_state = 80)\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\n# Assuming df_features is already created and sorted by feature importance\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = RandomForestClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = RandomForestClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"RandomForest Accuracy with Top 20 Features 20/80:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.90      0.90      0.90      1708\n           1       0.94      0.94      0.94      2875\n\n    accuracy                           0.92      4583\n   macro avg       0.92      0.92      0.92      4583\nweighted avg       0.92      0.92      0.92      4583\n\nRandomForest Accuracy with Top 20 Features 20/80: 0.9271219725070914\n\n\n\n\nShow the code\n# Assuming df_features is already created and sorted by feature importance\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = tree.DecisionTreeClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"DecisionTreeClassifier Accuracy with Top 20 Features 20/80:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.86      0.87      1742\n           1       0.92      0.93      0.92      2841\n\n    accuracy                           0.90      4583\n   macro avg       0.90      0.90      0.90      4583\nweighted avg       0.90      0.90      0.90      4583\n\nDecisionTreeClassifier Accuracy with Top 20 Features 20/80: 0.9048658084224307\n\n\n\n\nShow the code\n# Correct the syntax for dropping columns and selecting the target variable\nX = dwellings_ml.drop(['before1980', 'yrbuilt', 'parcel'], axis=1) # Assuming you want to drop these columns\ny = dwellings_ml['before1980'] # Correct way to select the target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=80)\n\n# Initialize the XGBClassifier model\nxgb_model = XGBClassifier(objective='binary:logistic', n_estimators=500, learning_rate=0.3, random_state=43)\n\n# Fit the model\nxgb_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = xgb_model.predict(X_test)\n\n# Evaluation\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(\"Confusion Matrix:\") \nprint(conf_matrix)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'XGboost Accuracy 20/80: {accuracy}')\n\n\nConfusion Matrix:\n[[1557  150]\n [ 165 2711]]\nXGboost Accuracy 20/80: 0.9312677285620773\n\n\nHere I joined two data sets, the Neighborhood dwellings and dwellings ml.\n\n\nShow the code\njoin = df.join(df2, lsuffix=\"parcel\", rsuffix=\"parcel\")\n# print(\"This is the joined dataframe of Neighborhood dwellings and dwellings ml. \")\n\n\n\n\nShow the code\nX_pred = join.drop(join.filter(regex = 'parcelparcel|parce|yrbuilt|before1980').columns, axis = 1)\ny_pred = join.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .34, random_state = 66)\n\n    # Extract the top 10 feature names\n\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\n# # clf is a classifier.\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n# #linear regression, \n# clf = RandomForestClassifier()\nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n\n# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)\n\n\n\n\nShow the code\ndf_features = pd.DataFrame(\n    {'f_names': X_train.columns, \n    'f_values': clf.feature_importances_}).sort_values('f_values', ascending = False)\n\n\n\n\nShow the code\ntop_20_features = df_features.head(110)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = RandomForestClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = RandomForestClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"RandomForest Accuracy with Top 20 Features 34/66:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.94      0.94      0.94      2906\n           1       0.96      0.96      0.96      4885\n\n    accuracy                           0.96      7791\n   macro avg       0.95      0.95      0.95      7791\nweighted avg       0.96      0.96      0.96      7791\n\nRandomForest Accuracy with Top 20 Features 34/66: 0.9546913104864587\n\n\n\n\nShow the code\n#%%\n# chart = px.bar(df_features.head(20), #You can change this 10 based on top features you want to see.\n#     x='f_values', \n#     y='f_names'\n# )\n\n# chart.update_layout(yaxis={'categoryorder':'total ascending'})\n# 0-1 is based on % 1 is 100% \n# f(x) = y (y is target, does not change here)\n# f is algorithm, Try 4\n# (x) is features \n\n\n\n\nShow the code\n# top_20_features = df_features.head(150)['f_names'].tolist()\n#For XGboost using all of the data produces a better result\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = tree.DecisionTreeClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = tree.DecisionTreeClassifier()#Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"Decision Tree Accuracy with Top 110 Features in Joined Data 34/66:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.92      0.93      0.93      2895\n           1       0.96      0.96      0.96      4896\n\n    accuracy                           0.95      7791\n   macro avg       0.94      0.94      0.94      7791\nweighted avg       0.95      0.95      0.95      7791\n\nDecision Tree Accuracy with Top 110 Features in Joined Data 34/66: 0.943139519958927\n\n\n\n\nShow the code\n# Correct the syntax for dropping columns and selecting the target variable\nX = join.drop(join.filter(regex = 'parcelparcel|parce|yrbuilt|before1980').columns, axis = 1)\ny = join.filter(regex = \"before1980\") # Assuming you want to drop these columns\n# Correct way to select the target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.34, random_state=66)\n\n# Initialize the XGBClassifier model\nxgb_model = XGBClassifier(objective='binary:logistic', n_estimators=500, learning_rate=0.34, random_state=66)\n\n# Fit the model\nxgb_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = xgb_model.predict(X_test)\n\n# Evaluation\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"This matrix shows the true positive, false positive, true negative, and false negative predictions.\")\nprint(\"Confusion Matrix:\") \nprint(conf_matrix)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'XGboost Accuracy with Joined Data Frame 34/66: {accuracy}')\n\n\nThis matrix shows the true positive, false positive, true negative, and false negative predictions.\nConfusion Matrix:\n[[2747  158]\n [ 140 4746]]\nXGboost Accuracy with Joined Data Frame 34/66: 0.9617507380310615\n\n\nThis next set of data shows how changing the test from 34|66 to 30|70 affected the accuracy. I found thatt accuracy would go up by .5-1.5% depending on the learning and algorithm.\n\n\nShow the code\nX_pred = join.drop(join.filter(regex = 'parcelparcel|parce|yrbuilt|before1980').columns, axis = 1)\ny_pred = join.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .30, random_state = 70)\n\n    # Extract the top 10 feature names\n\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\ntop_20_features = df_features.head(110)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = RandomForestClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = RandomForestClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"RandomForest Accuracy with Top 20 Features 34/66:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.95      0.94      0.94      2621\n           1       0.96      0.97      0.96      4253\n\n    accuracy                           0.96      6874\n   macro avg       0.95      0.95      0.95      6874\nweighted avg       0.96      0.96      0.96      6874\n\nRandomForest Accuracy with Top 20 Features 34/66: 0.9563572883328484\n\n\n\n\nShow the code\n# top_20_features = df_features.head(150)['f_names'].tolist()\n#For XGboost using all of the data produces a better result\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = tree.DecisionTreeClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = tree.DecisionTreeClassifier()#Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"Decision Tree Accuracy with Top 110 Features in Joined Data 30/70:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.94      0.92      0.93      2639\n           1       0.95      0.96      0.96      4235\n\n    accuracy                           0.95      6874\n   macro avg       0.94      0.94      0.94      6874\nweighted avg       0.95      0.95      0.95      6874\n\nDecision Tree Accuracy with Top 110 Features in Joined Data 30/70: 0.9434099505382602\n\n\n\n\nShow the code\n# Correct the syntax for dropping columns and selecting the target variable\nX = join.drop(join.filter(regex = 'parcelparcel|parce|yrbuilt|before1980').columns, axis = 1)\ny = join.filter(regex = \"before1980\") # Assuming you want to drop these columns\n# Correct way to select the target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=70)\n\n# Initialize the XGBClassifier model\nxgb_model = XGBClassifier(objective='binary:logistic', n_estimators=500, learning_rate=0.30, random_state=70)\n\n# Fit the model\nxgb_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = xgb_model.predict(X_test)\n\n# Evaluation\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"This matrix shows the true positive, false positive, true negative, and false negative predictions.\")\nprint(\"Confusion Matrix:\") \nprint(conf_matrix)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'XGboost Accuracy with Joined Data Frame 30/70: {accuracy}')\n\n\nThis matrix shows the true positive, false positive, true negative, and false negative predictions.\nConfusion Matrix:\n[[2470  134]\n [ 100 4170]]\nXGboost Accuracy with Joined Data Frame 30/70: 0.9659586848996218\n\n\nFor this set I am using the 80/20 test and with this the accuracy went back up. The highest was XGboost reaching 96.5% accuracy. \n\n\nShow the code\nX_pred = join.drop(join.filter(regex = 'parcelparcel|parce|yrbuilt|before1980').columns, axis = 1)\ny_pred = join.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .20, random_state = 80)\n\n    # Extract the top 10 feature names\n\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\ntop_20_features = df_features.head(110)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = RandomForestClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = RandomForestClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"RandomForest Accuracy with Top 20 Features 20/80:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.95      0.93      0.94      1740\n           1       0.96      0.97      0.96      2843\n\n    accuracy                           0.95      4583\n   macro avg       0.95      0.95      0.95      4583\nweighted avg       0.95      0.95      0.95      4583\n\nRandomForest Accuracy with Top 20 Features 20/80: 0.9578878463888283\n\n\n\n\nShow the code\n# top_20_features = df_features.head(150)['f_names'].tolist()\n#For XGboost using all of the data produces a better result\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = tree.DecisionTreeClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = tree.DecisionTreeClassifier()#Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"Decision Tree Accuracy with Top 110 Features in Joined Data 20/80:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.95      0.92      0.93      1752\n           1       0.95      0.97      0.96      2831\n\n    accuracy                           0.95      4583\n   macro avg       0.95      0.94      0.95      4583\nweighted avg       0.95      0.95      0.95      4583\n\nDecision Tree Accuracy with Top 110 Features in Joined Data 20/80: 0.9467597643464979\n\n\n\n\nShow the code\n# Correct the syntax for dropping columns and selecting the target variable\nX = join.drop(join.filter(regex = 'parcelparcel|parce|yrbuilt|before1980').columns, axis = 1)\ny = join.filter(regex = \"before1980\") # Assuming you want to drop these columns\n# Correct way to select the target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=80)\n\n# Initialize the XGBClassifier model\nxgb_model = XGBClassifier(objective='binary:logistic', n_estimators=500, learning_rate=0.20, random_state=80)\n\n# Fit the model\nxgb_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = xgb_model.predict(X_test)\n\n# Evaluation\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"This matrix shows the true positive, false positive, true negative, and false negative predictions.\")\nprint(\"Confusion Matrix:\") \nprint(conf_matrix)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'XGboost Accuracy with Joined Data Frame 20/80: {accuracy}')\n\n\nThis matrix shows the true positive, false positive, true negative, and false negative predictions.\nConfusion Matrix:\n[[1629   78]\n [  79 2797]]\nXGboost Accuracy with Joined Data Frame 20/80: 0.9657429631245908",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-3",
    "href": "Projects/project4.html#questiontask-3",
    "title": "Client Report - [Prediction of Houses Built Before 1980 in Colorado]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.\nThis Chart shows the features in order of percent influence on learning model. This is with the joined Data frame and XGboost. One Story Houses had the largest influence by over 15%. After this each features drops in importance to well below 1 percent. However, with XGboost I found that using all of the features produced the best result.\n\n\nShow the code\nX_pred = join.drop(join.filter(regex = 'parcelparcel|parce|yrbuilt|before1980').columns, axis = 1)\ny_pred = join.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .20, random_state = 80)\n\n    # Extract the top 10 feature names\n\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\n# Correct the syntax for dropping columns and selecting the target variable\nX = join.drop(join.filter(regex = 'parcelparcel|parce|yrbuilt|before1980').columns, axis = 1)\ny = join.filter(regex = \"before1980\") # Assuming you want to drop these columns\n# Correct way to select the target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=80)\n\n# Initialize the XGBClassifier model\nxgb_model = XGBClassifier(objective='binary:logistic', n_estimators=500, learning_rate=0.20, random_state=80)\n\n# Fit the model\nxgb_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = xgb_model.predict(X_test)\n\n\n\n\nShow the code\n# # clf is a classifier.\nclf = XGBClassifier() #Rand Forest is similar, try it. \n# #linear regression, \n# clf = RandomForestClassifier()\nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n\n# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)\n\n\n\n\nShow the code\ndf_features = pd.DataFrame(\n    {'f_names': X_train.columns, \n    'f_values': clf.feature_importances_}).sort_values('f_values', ascending = False)\n\n\n\n\nRead and format data\n# Include and execute your code here\n\n#%%\nchart = px.bar(df_features.head(25), #You can change this 10 based on top features you want to see.\n    x='f_values', \n    y='f_names'\n)\n\nchart.update_layout(yaxis={'categoryorder':'total ascending'})\n# 0-1 is based on % 1 is 100% \n# f(x) = y (y is target, does not change here)\n# f is algorithm, Try 4\n# (x) is features",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-4",
    "href": "Projects/project4.html#questiontask-4",
    "title": "Client Report - [Prediction of Houses Built Before 1980 in Colorado]",
    "section": "Question|Task 4",
    "text": "Question|Task 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\nThe First Evaluation Metric is True positive. A true positive represents a correct positive prediction made by the classification model. Below you can see the Matrix for XGboost. True Positive is the Top Left of the confusion matrix. False Positive is the top Right corner of the Matrix. This is where the model predicted something that was incorrect.\n\n\nShow the code\n# Correct the syntax for dropping columns and selecting the target variable\nX = join.drop(join.filter(regex = 'parcelparcel|parce|yrbuilt|before1980').columns, axis = 1)\ny = join.filter(regex = \"before1980\") # Assuming you want to drop these columns\n# Correct way to select the target variable\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=80)\n\n# Initialize the XGBClassifier model\nxgb_model = XGBClassifier(objective='binary:logistic', n_estimators=500, learning_rate=0.20, random_state=80)\n\n# Fit the model\nxgb_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = xgb_model.predict(X_test)\n\n# Evaluation\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"This matrix shows the true positive, false positive, true negative, and false negative predictions.\")\nprint(\"Confusion Matrix:\") \nprint(conf_matrix)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'XGboost Accuracy with Joined Data Frame 20/80: {accuracy}')\n\n\nThis matrix shows the true positive, false positive, true negative, and false negative predictions.\nConfusion Matrix:\n[[1629   78]\n [  79 2797]]\nXGboost Accuracy with Joined Data Frame 20/80: 0.9657429631245908\n\n\nThis is a relationship between Recall and Precision. High precision means the model is cautious and avoids false positives While High recall means the model captures most positive cases, minimizing false negatives. Both of these metrics need to be balanced as they can impede on the other.\nPrecision: QUALITY—how often positive predictions are correct. Recall: QUANTITY—how well the model finds all positive instances.",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project2.html",
    "href": "Projects/project2.html",
    "title": "Client Report - [Airline Delays by Airport for the U.S]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\nurl_flights = 'https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json'\nflights = pd.read_json(url_flights)\n# replace 999 and -999 with the mean, 1500+ replace w/ 1500. \n# \n\n\n\n\nShow the code\n# %%\npd.crosstab(\n    flights.month, \n    flights.airport_code)\n\n\n\n\n\n\n\n\nairport_code\nATL\nDEN\nIAD\nORD\nSAN\nSFO\nSLC\n\n\nmonth\n\n\n\n\n\n\n\n\n\n\n\nApril\n11\n11\n11\n11\n11\n11\n11\n\n\nAugust\n11\n10\n11\n11\n11\n11\n10\n\n\nDecember\n10\n10\n10\n11\n10\n11\n11\n\n\nFebuary\n10\n11\n11\n11\n11\n11\n11\n\n\nJanuary\n11\n10\n10\n11\n11\n10\n10\n\n\nJuly\n11\n11\n11\n11\n11\n11\n11\n\n\nJune\n11\n11\n11\n11\n10\n10\n11\n\n\nMarch\n10\n11\n11\n10\n8\n11\n10\n\n\nMay\n11\n11\n11\n9\n11\n10\n10\n\n\nNovember\n10\n11\n11\n11\n11\n11\n11\n\n\nOctober\n11\n11\n11\n11\n11\n11\n11\n\n\nSeptember\n11\n11\n10\n11\n10\n11\n10\n\n\nn/a\n4\n3\n3\n3\n6\n3\n5\n\n\n\n\n\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#elevator-pitch",
    "href": "Projects/project2.html#elevator-pitch",
    "title": "Client Report - [Airline Delays by Airport for the U.S]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\nurl_flights = 'https://github.com/byuidatascience/data4missing/raw/master/data-raw/flights_missing/flights_missing.json'\nflights = pd.read_json(url_flights)\n# replace 999 and -999 with the mean, 1500+ replace w/ 1500. \n# \n\n\n\n\nShow the code\n# %%\npd.crosstab(\n    flights.month, \n    flights.airport_code)\n\n\n\n\n\n\n\n\nairport_code\nATL\nDEN\nIAD\nORD\nSAN\nSFO\nSLC\n\n\nmonth\n\n\n\n\n\n\n\n\n\n\n\nApril\n11\n11\n11\n11\n11\n11\n11\n\n\nAugust\n11\n10\n11\n11\n11\n11\n10\n\n\nDecember\n10\n10\n10\n11\n10\n11\n11\n\n\nFebuary\n10\n11\n11\n11\n11\n11\n11\n\n\nJanuary\n11\n10\n10\n11\n11\n10\n10\n\n\nJuly\n11\n11\n11\n11\n11\n11\n11\n\n\nJune\n11\n11\n11\n11\n10\n10\n11\n\n\nMarch\n10\n11\n11\n10\n8\n11\n10\n\n\nMay\n11\n11\n11\n9\n11\n10\n10\n\n\nNovember\n10\n11\n11\n11\n11\n11\n11\n\n\nOctober\n11\n11\n11\n11\n11\n11\n11\n\n\nSeptember\n11\n11\n10\n11\n10\n11\n10\n\n\nn/a\n4\n3\n3\n3\n6\n3\n5\n\n\n\n\n\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-1",
    "href": "Projects/project2.html#questiontask-1",
    "title": "Client Report - [Airline Delays by Airport for the U.S]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”)\ntype your results and analysis here\n\n\nShow the code\n# %%\ndef missing_checks(df, column ):\n    out1 = df[column].isnull().sum(axis = 0)\n    out2 = df[column].describe()\n    out3 = df[column].describe(exclude=np.number)\n    print('\\n\\n\\n')\n    print('Checking column' + column)\n    print('\\n')\n    print('Missing summary')\n    print(out1)\n    print('\\n')\n    print(\"Numeric summaries\")\n    print(out2)\n    print('\\n')\n    print('Non Numeric summaries')\n    print(out3)\n\nmissing_checks(flights, 'num_of_delays_nas')\nmissing_checks(flights, 'num_of_delays_late_aircraft')\nmissing_checks(flights, 'num_of_delays_weather')    \n\n\n\n\n\n\nChecking columnnum_of_delays_nas\n\n\nMissing summary\n0\n\n\nNumeric summaries\ncount     924.000000\nmean     1376.467532\nstd      1348.719957\nmin        61.000000\n25%       357.750000\n50%       960.000000\n75%      1869.250000\nmax      8704.000000\nName: num_of_delays_nas, dtype: float64\n\n\nNon Numeric summaries\ncount     924.000000\nmean     1376.467532\nstd      1348.719957\nmin        61.000000\n25%       357.750000\n50%       960.000000\n75%      1869.250000\nmax      8704.000000\nName: num_of_delays_nas, dtype: float64\n\n\n\n\nChecking columnnum_of_delays_late_aircraft\n\n\nMissing summary\n0\n\n\nNumeric summaries\ncount     924.000000\nmean     1017.844156\nstd       853.942405\nmin      -999.000000\n25%       488.750000\n50%       804.000000\n75%      1473.750000\nmax      3969.000000\nName: num_of_delays_late_aircraft, dtype: float64\n\n\nNon Numeric summaries\ncount     924.000000\nmean     1017.844156\nstd       853.942405\nmin      -999.000000\n25%       488.750000\n50%       804.000000\n75%      1473.750000\nmax      3969.000000\nName: num_of_delays_late_aircraft, dtype: float64\n\n\n\n\nChecking columnnum_of_delays_weather\n\n\nMissing summary\n0\n\n\nNumeric summaries\ncount    924.000000\nmean     100.971861\nstd      103.584998\nmin        3.000000\n25%       34.750000\n50%       66.000000\n75%      129.000000\nmax      812.000000\nName: num_of_delays_weather, dtype: float64\n\n\nNon Numeric summaries\ncount    924.000000\nmean     100.971861\nstd      103.584998\nmin        3.000000\n25%       34.750000\n50%       66.000000\n75%      129.000000\nmax      812.000000\nName: num_of_delays_weather, dtype: float64\n\n\n\n\nShow the code\n# %%\nflights.num_of_delays_weather.describe()\n\n\ncount    924.000000\nmean     100.971861\nstd      103.584998\nmin        3.000000\n25%       34.750000\n50%       66.000000\n75%      129.000000\nmax      812.000000\nName: num_of_delays_weather, dtype: float64\n\n\n\n\nShow the code\n# %% assign function lets each of these columns get added to the data frame. \n# makes the new column = to another column in the df. (Like excel)\n# So minutes total delays will be another affed colimn at the end (severe)\n# lambda just returns whatever you do to x, it is an inline function. \n# weather = (flight_delays.assign(\n#     severe = flight_delays.num_of_delays_weather # no# missing, severe \n#     nodla_nona = lambda x: (x.num_of_delays_late_aircraft\n#         .replace(-999, np.nan)), #missing is -999, this is taking any missing data and replacing it with nan, copies the data but gives nan for 999\n#     mild_late = lambda x: x.nodla_nona.fillna(x.nodla_nona.mean())*0.3,\n#     mild = np.where(\n#         flight_delays.month.isin(['April', 'May', 'June', 'July', 'August']), \n#             flight_delays.num_of_delays_nas*0.4, \n#             flight_delays.num_of_delays_nas*0.65),\n#     weather = lambda x: x.severe + x.mild_late + x.mild,\n#     proportion_weather_delay = lambda x: x.weather / x.num_of_delays_total,\n#     proportion_weather_total = lambda x:  x.weather / x.num_of_flights_total)\n#     .filter(['airport_code','month','year', 'severe','mild', 'mild_late',\n#     'weather', 'proportion_weather_total', \n#     'proportion_weather_delay', 'num_of_flights_total', 'num_of_delays_total']))\n# weather.head()\n# weather = (flight_delays.assign(\n#     severe = flight_delays.num_of_delays_weather, # no# missing, severe \n#     nodla_nona = lambda x: (x.num_of_delays_late_aircraft\n#         .replace(-999, np.nan)), #missing is -999, this is taking any missing data and replacing it with nan, copies the data but gives nan for 999\n#     mild_late = lambda x: x.nodla_nona.fillna(x.nodla_nona.mean())*0.3,\n#     mild = np.where(\n#         flight_delays.month.isin(['April', 'May', 'June', 'July', 'August']), \n#             flight_delays.num_of_delays_nas*0.4, \n#             flight_delays.num_of_delays_nas*0.65),\n#     weather = lambda x: x.severe + x.mild_late + x.mild,\n#     proportion_weather_delay = lambda x: x.weather / x.num_of_delays_total,\n#     proportion_weather_total = lambda x:  x.weather / x.num_of_flights_total)\n#     .filter(['airport_code','month','year', 'severe','mild', 'mild_late',\n#     'weather', 'proportion_weather_total', \n#     'proportion_weather_delay', 'num_of_flights_total', 'num_of_delays_total']))\n# weather.head()\n\n\ninclude figures in chunks and discuss your findings in the figure.",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-2",
    "href": "Projects/project2.html#questiontask-2",
    "title": "Client Report - [Airline Delays by Airport for the U.S]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nWhich airport has the worst delays?\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\n```\ninclude figures in chunks and discuss your findings in the figure.",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-3",
    "href": "Projects/project2.html#questiontask-3",
    "title": "Client Report - [Airline Delays by Airport for the U.S]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nWhat is the best month to fly if you want to avoid delays of any length?\ninclude figures in chunks and discuss your findings in the figure.",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-4",
    "href": "Projects/project2.html#questiontask-4",
    "title": "Client Report - [Airline Delays by Airport for the U.S]",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nYour job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild).",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-5",
    "href": "Projects/project2.html#questiontask-5",
    "title": "Client Report - [Airline Delays by Airport for the U.S]",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Discuss what you learn from this graph.",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Projects/project1.html",
    "href": "Projects/project1.html",
    "title": "Client Report - [Usage of Names over time]",
    "section": "",
    "text": "With this data we can see the change in popularity of names over time. An example of this is with the names Mary, Martha, Paul and Peter, they lost popularity by an average of 31.25% per year from 1957 to the year 2000. \n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#elevator-pitch",
    "href": "Projects/project1.html#elevator-pitch",
    "title": "Client Report - [Usage of Names over time]",
    "section": "",
    "text": "With this data we can see the change in popularity of names over time. An example of this is with the names Mary, Martha, Paul and Peter, they lost popularity by an average of 31.25% per year from 1957 to the year 2000. \n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-1",
    "href": "Projects/project1.html#questiontask-1",
    "title": "Client Report - [Usage of Names over time]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nHow does your name at your birth year compare to its use historically?\nIn the year 1947 there were only 5 children named Darren. The name started to gain popularity in 1958 with 385 kids given the name and peaking at 5931 children given this name in 1965. I was born in the year 1996 where the popularity of the name Darren had declined by 81% from 1947. \n\n\nRead and format data\n# Include and execute your code here\n #and \"va != 0\"])\n# df.query(\"year &lt;= 2000\") \n\n# year with lowest babies names Darren\n# pd.unique(df.query(\"name == 'Darren' \").year).min()\n# pd.unique(df.query(\"name == 'Darren' \").year).max()\n\n# Gives the size, so rows times columns so with year, how many years you have, name: how many times the specified name occurs. \n# pd.unique(df.query(\"name == 'Darren' \").year).size\ndf.query(\"name == 'Darren'\").filter([\"name\", \"year\", \"Total\"]).query(\"year &gt; 1990 and year &lt; 2000\")\n\n\n\n\n\n\n\n\n\nname\nyear\nTotal\n\n\n\n\n90941\nDarren\n1991\n1277.0\n\n\n90942\nDarren\n1992\n1300.0\n\n\n90943\nDarren\n1993\n1419.0\n\n\n90944\nDarren\n1994\n1431.0\n\n\n90945\nDarren\n1995\n1271.0\n\n\n90946\nDarren\n1996\n1109.0\n\n\n90947\nDarren\n1997\n989.0\n\n\n90948\nDarren\n1998\n997.0\n\n\n90949\nDarren\n1999\n1001.0\n\n\n\n\n\n\n\nThe green line indicates when I was born. \n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot example\n# Include and execute your code here\ndarren_df = df[df['name'] == 'Darren']\nchart = px.bar(darren_df.head(200),\n    x=\"year\", \n    y=\"Total\",\n    title = \"Usage of the name Darren\",\n    labels = {\"year\":\"Year\"}\n\n)\nhighlight_year = 1996\n\n# Add a vertical line\nchart.add_shape(\n    type=\"line\",\n    x0=highlight_year,\n    y0=darren_df['Total'].min(),\n    x1=highlight_year,\n    y1=darren_df['Total'].max(),\n    line=dict(\n        color=\"green\",\n        width=3,\n        dash=\"dashdot\"\n    )\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-2",
    "href": "Projects/project1.html#questiontask-2",
    "title": "Client Report - [Usage of Names over time]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nI would guess they would be born between the years 1985 and 1996. Which would be between the ages of 27 and 38. The reason why I would choose this range of age is because the majority of people named brittany were born in these years. 241847 out of 327718 were born in these years. That is 73.8% of people named Brittany in a 11 year period.I would not choose any date from 1968 - 1984 and 1997 and 2015 because that is only 26.2 % of people name Brittany were born in these years. \n\n\nRead and format data\n# Include and execute your code here\n\ndf.query(\"name == 'Brittany'\").filter([\"name\", \"year\",\n \"Total\"]).query(\"year &gt; 1984 and year &lt; 1997\")\n\n\n\n\n\n\n\n\n\nname\nyear\nTotal\n\n\n\n\n53222\nBrittany\n1985\n14010.0\n\n\n53223\nBrittany\n1986\n17856.5\n\n\n53224\nBrittany\n1987\n18825.5\n\n\n53225\nBrittany\n1988\n21952.0\n\n\n53226\nBrittany\n1989\n30848.0\n\n\n53227\nBrittany\n1990\n32562.5\n\n\n53228\nBrittany\n1991\n26963.5\n\n\n53229\nBrittany\n1992\n23416.5\n\n\n53230\nBrittany\n1993\n21728.0\n\n\n53231\nBrittany\n1994\n17808.5\n\n\n53232\nBrittany\n1995\n15875.5\n\n\n53233\nBrittany\n1996\n13796.0\n\n\n\n\n\n\n\nThis is isolating the years that hold 73.8% of people named Brittany\n::: {#cell-Q2 chart .cell execution_count=6}\n\nplot example\n# Include and execute your code here\nfiltered_df = df[df['name'] == 'Brittany']\nchart = px.bar(filtered_df.head(200),\n    x=\"year\", \n    y=\"Total\",\n    title = \"Brittany name usage over the years\"\n)\nchart.add_shape(\n    type=\"line\",\n    x0= 1985,\n    y0=14000,\n    x1=1996,\n    y1=14000,\n    line=dict(\n        color=\"red\",\n        width=3,\n        dash=\"dashdot\")\n        )\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=7}\n\ntable example\n# Include and execute your code here\nmydat = filtered_df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n38\n2006\n0.0\n13.0\n\n\n39\n2007\n0.0\n12.0\n\n\n40\n2008\n0.0\n0.0\n\n\n41\n2009\n0.0\n6.0\n\n\n42\n2010\n0.0\n0.0\n\n\n43\n2011\n0.0\n7.0\n\n\n44\n2012\n0.0\n13.0\n\n\n45\n2013\n0.0\n0.0\n\n\n46\n2014\n0.0\n5.0\n\n\n47\n2015\n0.0\n6.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-3",
    "href": "Projects/project1.html#questiontask-3",
    "title": "Client Report - [Usage of Names over time]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names. What trends do you notice?\n_These four names follow the same trend line in popularity over time. This may be due to cultural shifts in the popularity of christianity. Between 1947 and 1957 was the peak of usage of these names with an average usage of 201149 This drops relativly quickly over the years with an average percent decrese of 31.45% over 43 years.\n::: {#cell-Q3 chart .cell execution_count=8}\n\nplot example\n# Include and execute your code here\ndesired_names = [\"Mary\", \"Martha\", \"Peter\", \"Paul\"]\n#List of names I want to see in chart\n#isin is boolean check. Is the four names in the column?\n# filter_df = df.name.isin(desired_names)\n# filter_df = df[df[\"name\"].isin(desired_names)]\nfilter_df = df[df.name.isin(desired_names)].query(\"year &gt; 1919 and year &lt; 2001\")\n#takes filter and query filter_df variable and applies to the chart. \nchart = px.bar(filter_df,\n    x=\"year\", \n    y=\"Total\",\n    color = \"name\",\n    title = \"Comparison of Martha, Mary, Peter and Paul over the years\",\n    labels = {\"year\": \"Year\"}\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=9}\n\ntable example\n# Include and execute your code here\n\n# Taking instances of each record and adding them together.\nmydat = (df\n    [df.name.isin(desired_names)]\n    .query('year &gt; 1947 and year &lt; 1957')\n    .groupby('name')\n    .agg(total = ('Total', 'sum'))\n    .reset_index()\n)\n\nmydat.head(50)\n\n\n\n# mydat = filter_df.head(1000)\\\n#     .groupby('year')\\\n#     .sum()\\\n#     .reset_index()\\\n#     .tail(10)\\\n#     .filter([\"year\", \"AK\",\"AR\"])\n\n# display(mydat)\n\n\n\n\n\n\n\n\n\nname\ntotal\n\n\n\n\n0\nMartha\n82131.0\n\n\n1\nMary\n437024.5\n\n\n2\nPaul\n203619.0\n\n\n3\nPeter\n81825.5\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-4",
    "href": "Projects/project1.html#questiontask-4",
    "title": "Client Report - [Usage of Names over time]",
    "section": "Question|Task 4",
    "text": "Question|Task 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\n_I chose the name Allison from Raiders of the Lost Ark. This movie released in 1981, The name began to gain popularity and over 4 years increase by 43.36%. The movie was the most popular movie of 1981 and so would potentially affect the usage of names over the next couple of years.\n\n\nShow the code\nmovie_df = df[df['name'] == 'Allison']\nchart = px.bar(movie_df.head(200),\n    x=\"year\", \n    y=\"Total\",\n    title = \"Allison name usage over the years\",\n    labels = {\"year\": \"Year\"}\n)\nchart.add_shape(\n    type=\"line\",\n    x0= 1981,\n    y0=4990,\n    x1=1984,\n    y1=7000,\n    line=dict(\n        color=\"red\",\n        width=3,\n        dash=\"dashdot\")\n        )\nchart.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project3.html",
    "href": "Projects/project3.html",
    "title": "Client Report - [Baseball Over the Years]",
    "section": "",
    "text": "Show the code\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\nq = 'SELECT * FROM allstarfull LIMIT 5'\nresults = pd.read_sql_query(q,con)\n\n# results",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#elevator-pitch",
    "href": "Projects/project3.html#elevator-pitch",
    "title": "Client Report - [Baseball Over the Years]",
    "section": "Elevator pitch",
    "text": "Elevator pitch\nWhen considering baseball players it is important to look at their average hits. Within this average we see that the number of total hits and At bats is very important for choosing players for a team. Somebody with a couple thousand at bats but a lower % is better than someone with 10 at bats and 60% hit rate.",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-1",
    "href": "Projects/project3.html#questiontask-1",
    "title": "Client Report - [Baseball Over the Years]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nI made sure to only display unique values in this table, meaning I used distinct to ensure there were no repeating values and it is ordered by salary. We can see that there are only 2 players who had attented byui. One thing to note is that as these two players gained experience their salaries increased. Lindsman though lost some pay in 2013 comapared to his previous 2 years.\n\n\nShow the code\nq = '''SELECT DISTINCT cp.playerID, cp.schoolID, s.salary, s.yearID, s.teamID\nFROM collegeplaying cp\nJOIN salaries s ON cp.playerID = s.playerID\nWHERE cp.schoolID = 'idbyuid'\nORDER BY s.salary DESC'''\n\nresult = pd.read_sql_query(q, con)\n\nprint(result)\n\n\n     playerID schoolID     salary  yearID teamID\n0   lindsma01  idbyuid  4000000.0    2014    CHA\n1   lindsma01  idbyuid  3600000.0    2012    BAL\n2   lindsma01  idbyuid  2800000.0    2011    COL\n3   lindsma01  idbyuid  2300000.0    2013    CHA\n4   lindsma01  idbyuid  1625000.0    2010    HOU\n5   stephga01  idbyuid  1025000.0    2001    SLN\n6   stephga01  idbyuid   900000.0    2002    SLN\n7   stephga01  idbyuid   800000.0    2003    SLN\n8   stephga01  idbyuid   550000.0    2000    SLN\n9   lindsma01  idbyuid   410000.0    2009    FLO\n10  lindsma01  idbyuid   395000.0    2008    FLO\n11  lindsma01  idbyuid   380000.0    2007    FLO\n12  stephga01  idbyuid   215000.0    1999    SLN\n13  stephga01  idbyuid   185000.0    1998    PHI\n14  stephga01  idbyuid   150000.0    1997    PHI",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-2",
    "href": "Projects/project3.html#questiontask-2",
    "title": "Client Report - [Baseball Over the Years]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\nWrite an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\nUse the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\nNow calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\n_The top hitters in this situation have an average hit rate of 100%. If you look in the H (hits) row you will see they have only hit once, as well as been at bat once.\n\n\nShow the code\nq = '''\nSELECT playerID, yearID, H, AB, (H * 100) / AB * 1.0 AS \"Average Hits %\"\nFROM batting\nWHERE AB &gt;= 1 \nGROUP BY playerID\nORDER BY H * 100/AB *1.0 DESC, playerID \nLIMIT 5'''\n\nresult = pd.read_sql_query(q, con)\n\nprint(result)\n\n\n    playerID  yearID  H  AB  Average Hits %\n0  abramge01    1923  1   1           100.0\n1  alanirj01    2019  1   1           100.0\n2  alberan01    2017  1   1           100.0\n3  allarko01    2018  1   1           100.0\n4  banisje01    1991  1   1           100.0\n\n\nWhen you take the At Bats and increase it to at least 10 times you see that the highest stat is 64% instead of 100% like the previous chart. These percentiles are much more useful as they have at least some data behind them. These top five batters all were at bat 11-16 times.\n\n\nShow the code\nq = '''\nSELECT playerID, yearID, H, AB, (H * 100) / AB * 1.0 AS \"Average Hits %\"\nFROM batting\nWHERE AB &gt;= 10 \nGROUP BY playerID\nORDER BY H * 100/AB *1.0 DESC \nLIMIT 5'''\n\nresult = pd.read_sql_query(q, con)\n\nprint(result)\n\n\n    playerID  yearID  H  AB  Average Hits %\n0  nymanny01    1974  9  14            64.0\n1  silvech01    1948  8  14            57.0\n2  puccige01    1930  9  16            56.0\n3  applepe01    1927  6  11            54.0\n4  ariasjo01    2006  6  11            54.0\n\n\n_In this last set of data you see the Average Hits go down even further from before. However, this is the most useful of the charts. All of these batters had well over 100 bats, meaning that they have many actual data points to use for their average hits. If you were to look at players you were interested in hiring for a team, the amount of Hits and at bats influencing the Average hits would be important to note. Concistency over long periods / many data points and average hits at 49% would be much more indicative of good hitters than someone who has been at bat 14 times and has a 64% average hit rate.\n\n\nShow the code\nq = \"\"\" \nSELECT playerID, sum(H), sum(AB), (SUM(H) * 100) / sum(AB) * 1.0 AS \"Average hits %\"\nFROM batting \nGROUP BY playerID \nHAVING SUM(AB) &gt;= 100\nORDER BY  sum(H) * 100/ sum(AB) *1.0 DESC\nLIMIT 5\n\"\"\"\nresult = pd.read_sql_query(q, con)\n\nprint(result) \n\n\n    playerID  sum(H)  sum(AB)  Average hits %\n0   cobbty01    4189    11436            36.0\n1  barnero01     860     2391            35.0\n2  hornsro01    2930     8173            35.0\n3  jacksjo01    1772     4981            35.0\n4   kingst01      96      272            35.0",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-3",
    "href": "Projects/project3.html#questiontask-3",
    "title": "Client Report - [Baseball Over the Years]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Plotly Express to visualize the comparison. What do you learn?\nPhilidelfia Phillies vs New York Yankies for 10 years. That is 2006 - 2016. I chose these two popular teams to see how they average over time. This chart shows their average Home Runs with the New York Yankies averaging higher by 2%.\n\n\nShow the code\nq= '''SELECT DISTINCT teamID, \n       SUM(HR) AS Total_Home_Runs, \n       SUM(H) AS Total_Hits, \n       SUM(AB) AS Total_At_Bats, \n       (SUM(HR) * 100.0) / SUM(H) AS \"Average Home Runs %\"\nFROM teams\nWHERE teamID IN ('PHI', 'NYA') AND yearID &gt;= 2006\nGROUP BY teamID\nORDER BY \"Average_Home_Runs_Percentage\" DESC'''\n\nresult = pd.read_sql_query(q, con)\n\nprint(result)\n\n\n  teamID  Total_Home_Runs  Total_Hits  Total_At_Bats  Average Home Runs %\n0    PHI             2475       19607          77718            12.623043\n1    NYA             3003       20554          77872            14.610295\n\n\nThis chart is looking at the total pay to players over ten years and the average pay of players over the same period of time. New York Yankes paid 2.26 billion dollars over 10 years to their players, this is 870million dollars more than the Phillidelphia Phillies\n\n\nShow the code\nq = '''SELECT teamID, \n              ROUND(SUM(salary), 3) AS Total_Player_Pay, \n              ROUND((SUM(salary) / COUNT(playerID)), 3) AS Average_Player_Salary\n       FROM salaries\n       WHERE teamID IN ('PHI', 'NYA') AND yearID &gt;= 2006\n       GROUP BY teamID\n       ORDER BY Average_Player_Salary DESC'''\n\n\nresult = pd.read_sql_query(q, con)\n\nprint(result)\n\n\n  teamID  Total_Player_Pay  Average_Player_Salary\n0    NYA      2.263671e+09            7349582.305\n1    PHI      1.399510e+09            4471278.990\n\n\nThese First Chart shown is the top 10 teams with highest total player pay over ten years and their average player salaries. The Second Chart is those same teams Average % Home Runs. When comparing these charts we see that the New York Yankies pay the most as well as have a 2% higher Home Run Average. However, the other top 8 teams are within 1.3% of each other in homeruns and pay does not seem to affect their results significantly in this area. Example is that Boston (BOS) has payed about 500 million more than the Chicogo White Socks (CHA) Yet BOS is in 5th place and CHA is in 2nd for Average Home Runs. A million dollar difference in average player salary between the two and BOS is .52% below on Home Runs. \n\n\nShow the code\n# Query to compare total player pay and average player salary for top 10 teams\nq_salary = '''\nSELECT teamID, \n       ROUND(SUM(salary), 3) AS Total_Player_Pay, \n       ROUND((SUM(salary) / COUNT(playerID)), 3) AS Average_Player_Salary\nFROM salaries\nWHERE yearID &gt;= 2006\nGROUP BY teamID\nORDER BY Average_Player_Salary DESC\nLIMIT 10\n'''\n\n# Query to compare average home runs percentage for selected teams\nq_home_runs = '''\nSELECT DISTINCT teamID, \n       SUM(HR) AS Total_Home_Runs, \n       SUM(H) AS Total_Hits, \n       SUM(AB) AS Total_At_Bats, \n       (SUM(HR) * 100.0) / SUM(H) AS \"Average Home Runs %\"\nFROM teams\nWHERE teamID IN ('NYA','BOS','LAN','DET','PHI','LAA','SFN','CHN','CHA','NYN') AND yearID &gt;= 2006\nGROUP BY teamID\nORDER BY \"Average Home Runs %\" DESC\n'''\n\n# Execute queries and store results in DataFrames\nresult_salary = pd.read_sql_query(q_salary, con)\nresult_home_runs = pd.read_sql_query(q_home_runs, con)\n\n# Display the results\nprint(\"Top 10 Teams by Total Player Pay and Average Player Salary:\")\nprint(result_salary)\nprint(\"\\nComparison of Average Home Runs Percentage for Selected Teams:\")\nprint(result_home_runs)\n\n\nTop 10 Teams by Total Player Pay and Average Player Salary:\n  teamID  Total_Player_Pay  Average_Player_Salary\n0    NYA      2.263671e+09            7349582.305\n1    BOS      1.675458e+09            5187175.192\n2    LAN      1.598053e+09            4902001.856\n3    DET      1.457055e+09            4840714.173\n4    PHI      1.399510e+09            4471278.990\n5    LAA      1.347061e+09            4387819.583\n6    SFN      1.315003e+09            4354313.089\n7    CHN      1.243140e+09            4214034.678\n8    CHA      1.186187e+09            4133055.666\n9    NYN      1.215769e+09            3973102.520\n\nComparison of Average Home Runs Percentage for Selected Teams:\n  teamID  Total_Home_Runs  Total_Hits  Total_At_Bats  Average Home Runs %\n0    NYA             3003       20554          77872            14.610295\n1    CHA             2544       19839          77372            12.823227\n2    PHI             2475       19607          77718            12.623043\n3    CHN             2441       19715          77357            12.381435\n4    BOS             2613       21239          78881            12.302839\n5    LAA             2382       20168          77379            11.810789\n6    NYN             2305       19593          77356            11.764406\n7    LAN             2300       19994          77067            11.503451\n8    DET             2411       20961          78357            11.502314\n9    SFN             1829       19735          77454             9.267798\n\n\nq_combined = ’’’\nThis Chart shows the relationship between average player salary and % home runs. I wanted to show not just PHA and NYA but the top ten results as well.\n\n\nShow the code\n# Execute the SQL queries and store results in DataFrames\nresult_salary = pd.read_sql_query(q_salary, con)\nresult_home_runs = pd.read_sql_query(q_home_runs, con)\n\n# Merge the two DataFrames on the 'teamID' column\nmerged_data = pd.merge(result_salary, result_home_runs, on='teamID', how='inner')\n\n# Define a dictionary to map colors to team IDs\ncolor_map = {'BOS': 'red', 'CHA': 'black'}\n\n\nfig = px.bar(merged_data, x='teamID', y=['Average_Player_Salary', 'Average Home Runs %'],\n             title='Comparison of Average Player Salary and Average Home Runs Percentage (2006-2016)',\n             labels={'value': 'Average Home Run', 'variable': 'Metric'},\n             hover_data={'Average_Player_Salary': True},\n             color='teamID',  # Assign colors based on teamID\n             color_discrete_map=color_map)  # Map team IDs to colors\n\n\nfig.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project5.html",
    "href": "Projects/project5.html",
    "title": "Client Report - [Star Wars comprehension]",
    "section": "",
    "text": "Show the code\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#elevator-pitch",
    "href": "Projects/project5.html#elevator-pitch",
    "title": "Client Report - [Star Wars comprehension]",
    "section": "Elevator pitch",
    "text": "Elevator pitch\npaste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nShow the code\nurl = 'https://github.com/fivethirtyeight/data/raw/master/star-wars-survey/StarWars.csv'\ndf_names = pd.read_csv(url, encoding = \"ISO-8859-1\", nrows = 1).melt()\ndf = pd.read_csv(url, encoding = \"ISO-8859-1\",skiprows =2, header = None )\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-1",
    "href": "Projects/project5.html#questiontask-1",
    "title": "Client Report - [Star Wars comprehension]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nShorten the column names and clean them up for easier use with pandas. Provide a table or list that exemplifies how you fixed the names.\ntype your results and analysis here\n\n\nShow the code\n# %%\n# this is not complete.\n#\\d stands for any numerical digit, 0-9 and 10-99 {1,2} howmany digits in a row so {1,3} is 1 - 999\ndf_names = (df_names\n   .replace('Unnamed: \\d{1,2}', np.nan, regex=True)\n   .replace('Response', \"\") #If you find response make it blank \"\"\n)\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nShow the code\ndf_names = (df_names\n   .replace('Unnamed: \\d{1,2}', np.nan, regex=True)\n   .replace('Response', \"\")\n   .assign(\n      clean_variable = lambda x: x.variable.str.strip()\n         .replace(\n            'Which of the following Star Wars films have you seen? Please select all that apply.','seen'),\n      clean_value = lambda x: x.value.str.strip()\n   )\n)\n\n\n\n\nShow the code\ndf_names = (df_names\n   .replace('Unnamed: \\d{1,2}', np.nan, regex=True)\n   .replace('Response', \"\")\n   .assign(\n      clean_variable = lambda x: x.variable.str.strip()\n         .replace(\n            'Which of the following Star Wars films have you seen? Please select all that apply.','seen'),\n      clean_value = lambda x: x.value.str.strip())\n   .fillna(method = 'ffill')\n   .assign(\n      column_name = lambda x: x.clean_variable.str.cat(x.clean_value, sep = \"__\"),\n   )\n)\n\n\n\n\nShow the code\n#%%\ndf_names\n\n\n\n\n\n\n\n\n\nvariable\nvalue\nclean_variable\nclean_value\ncolumn_name\n\n\n\n\n0\nRespondentID\nNaN\nRespondentID\nNaN\nNaN\n\n\n1\nHave you seen any of the 6 films in the Star W...\n\nHave you seen any of the 6 films in the Star W...\n\nHave you seen any of the 6 films in the Star W...\n\n\n2\nDo you consider yourself to be a fan of the St...\n\nDo you consider yourself to be a fan of the St...\n\nDo you consider yourself to be a fan of the St...\n\n\n3\nWhich of the following Star Wars films have yo...\nStar Wars: Episode I The Phantom Menace\nseen\nStar Wars: Episode I The Phantom Menace\nseen__Star Wars: Episode I The Phantom Menace\n\n\n4\nWhich of the following Star Wars films have yo...\nStar Wars: Episode II Attack of the Clones\nseen\nStar Wars: Episode II Attack of the Clones\nseen__Star Wars: Episode II Attack of the Clones\n\n\n5\nWhich of the following Star Wars films have yo...\nStar Wars: Episode III Revenge of the Sith\nseen\nStar Wars: Episode III Revenge of the Sith\nseen__Star Wars: Episode III Revenge of the Sith\n\n\n6\nWhich of the following Star Wars films have yo...\nStar Wars: Episode IV A New Hope\nseen\nStar Wars: Episode IV A New Hope\nseen__Star Wars: Episode IV A New Hope\n\n\n7\nWhich of the following Star Wars films have yo...\nStar Wars: Episode V The Empire Strikes Back\nseen\nStar Wars: Episode V The Empire Strikes Back\nseen__Star Wars: Episode V The Empire Strikes ...\n\n\n8\nWhich of the following Star Wars films have yo...\nStar Wars: Episode VI Return of the Jedi\nseen\nStar Wars: Episode VI Return of the Jedi\nseen__Star Wars: Episode VI Return of the Jedi\n\n\n9\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode I The Phantom Menace\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode I The Phantom Menace\nPlease rank the Star Wars films in order of pr...\n\n\n10\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode II Attack of the Clones\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode II Attack of the Clones\nPlease rank the Star Wars films in order of pr...\n\n\n11\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode III Revenge of the Sith\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode III Revenge of the Sith\nPlease rank the Star Wars films in order of pr...\n\n\n12\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode IV A New Hope\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode IV A New Hope\nPlease rank the Star Wars films in order of pr...\n\n\n13\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode V The Empire Strikes Back\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode V The Empire Strikes Back\nPlease rank the Star Wars films in order of pr...\n\n\n14\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode VI Return of the Jedi\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode VI Return of the Jedi\nPlease rank the Star Wars films in order of pr...\n\n\n15\nPlease state whether you view the following ch...\nHan Solo\nPlease state whether you view the following ch...\nHan Solo\nPlease state whether you view the following ch...\n\n\n16\nPlease state whether you view the following ch...\nLuke Skywalker\nPlease state whether you view the following ch...\nLuke Skywalker\nPlease state whether you view the following ch...\n\n\n17\nPlease state whether you view the following ch...\nPrincess Leia Organa\nPlease state whether you view the following ch...\nPrincess Leia Organa\nPlease state whether you view the following ch...\n\n\n18\nPlease state whether you view the following ch...\nAnakin Skywalker\nPlease state whether you view the following ch...\nAnakin Skywalker\nPlease state whether you view the following ch...\n\n\n19\nPlease state whether you view the following ch...\nObi Wan Kenobi\nPlease state whether you view the following ch...\nObi Wan Kenobi\nPlease state whether you view the following ch...\n\n\n20\nPlease state whether you view the following ch...\nEmperor Palpatine\nPlease state whether you view the following ch...\nEmperor Palpatine\nPlease state whether you view the following ch...\n\n\n21\nPlease state whether you view the following ch...\nDarth Vader\nPlease state whether you view the following ch...\nDarth Vader\nPlease state whether you view the following ch...\n\n\n22\nPlease state whether you view the following ch...\nLando Calrissian\nPlease state whether you view the following ch...\nLando Calrissian\nPlease state whether you view the following ch...\n\n\n23\nPlease state whether you view the following ch...\nBoba Fett\nPlease state whether you view the following ch...\nBoba Fett\nPlease state whether you view the following ch...\n\n\n24\nPlease state whether you view the following ch...\nC-3P0\nPlease state whether you view the following ch...\nC-3P0\nPlease state whether you view the following ch...\n\n\n25\nPlease state whether you view the following ch...\nR2 D2\nPlease state whether you view the following ch...\nR2 D2\nPlease state whether you view the following ch...\n\n\n26\nPlease state whether you view the following ch...\nJar Jar Binks\nPlease state whether you view the following ch...\nJar Jar Binks\nPlease state whether you view the following ch...\n\n\n27\nPlease state whether you view the following ch...\nPadme Amidala\nPlease state whether you view the following ch...\nPadme Amidala\nPlease state whether you view the following ch...\n\n\n28\nPlease state whether you view the following ch...\nYoda\nPlease state whether you view the following ch...\nYoda\nPlease state whether you view the following ch...\n\n\n29\nWhich character shot first?\n\nWhich character shot first?\n\nWhich character shot first?__\n\n\n30\nAre you familiar with the Expanded Universe?\n\nAre you familiar with the Expanded Universe?\n\nAre you familiar with the Expanded Universe?__\n\n\n31\nDo you consider yourself to be a fan of the Ex...\n\nDo you consider yourself to be a fan of the Ex...\n\nDo you consider yourself to be a fan of the Ex...\n\n\n32\nDo you consider yourself to be a fan of the St...\n\nDo you consider yourself to be a fan of the St...\n\nDo you consider yourself to be a fan of the St...\n\n\n33\nGender\n\nGender\n\nGender__\n\n\n34\nAge\n\nAge\n\nAge__\n\n\n35\nHousehold Income\n\nHousehold Income\n\nHousehold Income__\n\n\n36\nEducation\n\nEducation\n\nEducation__\n\n\n37\nLocation (Census Region)\n\nLocation (Census Region)\n\nLocation (Census Region)__\n\n\n\n\n\n\n\n\n\nShow the code\ndf.columns = df_names\ndf.head() #Apply columns back to data frame after some cleansing. \n\n\n\n\n\n\n\n\n\n(RespondentID, nan, RespondentID, nan, nan)\n(Have you seen any of the 6 films in the Star Wars franchise?, , Have you seen any of the 6 films in the Star Wars franchise?, , Have you seen any of the 6 films in the Star Wars franchise?__)\n(Do you consider yourself to be a fan of the Star Wars film franchise?, , Do you consider yourself to be a fan of the Star Wars film franchise?, , Do you consider yourself to be a fan of the Star Wars film franchise?__)\n(Which of the following Star Wars films have you seen? Please select all that apply., Star Wars: Episode I The Phantom Menace, seen, Star Wars: Episode I The Phantom Menace, seen__Star Wars: Episode I The Phantom Menace)\n(Which of the following Star Wars films have you seen? Please select all that apply., Star Wars: Episode II Attack of the Clones, seen, Star Wars: Episode II Attack of the Clones, seen__Star Wars: Episode II Attack of the Clones)\n(Which of the following Star Wars films have you seen? Please select all that apply., Star Wars: Episode III Revenge of the Sith, seen, Star Wars: Episode III Revenge of the Sith, seen__Star Wars: Episode III Revenge of the Sith)\n(Which of the following Star Wars films have you seen? Please select all that apply., Star Wars: Episode IV A New Hope, seen, Star Wars: Episode IV A New Hope, seen__Star Wars: Episode IV A New Hope)\n(Which of the following Star Wars films have you seen? Please select all that apply., Star Wars: Episode V The Empire Strikes Back, seen, Star Wars: Episode V The Empire Strikes Back, seen__Star Wars: Episode V The Empire Strikes Back)\n(Which of the following Star Wars films have you seen? Please select all that apply., Star Wars: Episode VI Return of the Jedi, seen, Star Wars: Episode VI Return of the Jedi, seen__Star Wars: Episode VI Return of the Jedi)\n(Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film., Star Wars: Episode I The Phantom Menace, Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film., Star Wars: Episode I The Phantom Menace, Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.__Star Wars: Episode I The Phantom Menace)\n...\n(Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her., Yoda, Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her., Yoda, Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.__Yoda)\n(Which character shot first?, , Which character shot first?, , Which character shot first?__)\n(Are you familiar with the Expanded Universe?, , Are you familiar with the Expanded Universe?, , Are you familiar with the Expanded Universe?__)\n(Do you consider yourself to be a fan of the Expanded Universe?Œæ, , Do you consider yourself to be a fan of the Expanded Universe?Œæ, , Do you consider yourself to be a fan of the Expanded Universe?Œæ__)\n(Do you consider yourself to be a fan of the Star Trek franchise?, , Do you consider yourself to be a fan of the Star Trek franchise?, , Do you consider yourself to be a fan of the Star Trek franchise?__)\n(Gender, , Gender, , Gender__)\n(Age, , Age, , Age__)\n(Household Income, , Household Income, , Household Income__)\n(Education, , Education, , Education__)\n(Location (Census Region), , Location (Census Region), , Location (Census Region)__)\n\n\n\n\n0\n3292879998\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\n18-29\nNaN\nHigh school degree\nSouth Atlantic\n\n\n1\n3292879538\nNo\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nYes\nMale\n18-29\n$0 - $24,999\nBachelor degree\nWest South Central\n\n\n2\n3292765271\nYes\nNo\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nNaN\nNaN\nNaN\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n18-29\n$0 - $24,999\nHigh school degree\nWest North Central\n\n\n3\n3292763116\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n4\n3292731220\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n\n\n5 rows × 38 columns\n\n\n\n\n\nShow the code\n# %%\n# Shorten the column names and clean them \n# up for easier use with pandas.\n# we want to use this with the .replace() command that accepts a dictionary.\nvariables_replace = {\n    'Which of the following Star Wars films have you seen\\\\? Please select all that apply\\\\.':'seen',\n    'Please rank the Star Wars films in order of preference with 1 being your favorite film in the franchise and 6 being your least favorite film.':'rank',\n    'Please state whether you view the following characters favorably, unfavorably, or are unfamiliar with him/her.':'view',\n    'Do you consider yourself to be a fan of the Star Trek franchise\\\\?':'star_trek_fan',\n    'Do you consider yourself to be a fan of the Expanded Universe\\\\?\\x8c&aelig;':'expanded_fan',\n    'Are you familiar with the Expanded Universe\\\\?':'know_expanded',\n    'Have you seen any of the 6 films in the Star Wars franchise\\\\?':'seen_any',\n    'Do you consider yourself to be a fan of the Star Wars film franchise\\\\?':'star_wars_fans',\n    'Which character shot first\\\\?':'shot_first',\n    'Unnamed: \\d{1,2}':np.nan,\n    ' ':'_',\n}\nvalues_replace = {\n    'Response':'',\n    'Star Wars: Episode ':'',\n    ' ':'_'\n}\n\n\n\n\nShow the code\n# %%\nprint(df_names.value)\ndf_names.value.str.strip().replace(values_replace, regex=True)\nprint(df_names.value)\n\n\n0                                              NaN\n1                                                 \n2                                                 \n3         Star Wars: Episode I  The Phantom Menace\n4      Star Wars: Episode II  Attack of the Clones\n5      Star Wars: Episode III  Revenge of the Sith\n6                Star Wars: Episode IV  A New Hope\n7     Star Wars: Episode V The Empire Strikes Back\n8         Star Wars: Episode VI Return of the Jedi\n9         Star Wars: Episode I  The Phantom Menace\n10     Star Wars: Episode II  Attack of the Clones\n11     Star Wars: Episode III  Revenge of the Sith\n12               Star Wars: Episode IV  A New Hope\n13    Star Wars: Episode V The Empire Strikes Back\n14        Star Wars: Episode VI Return of the Jedi\n15                                        Han Solo\n16                                  Luke Skywalker\n17                            Princess Leia Organa\n18                                Anakin Skywalker\n19                                  Obi Wan Kenobi\n20                               Emperor Palpatine\n21                                     Darth Vader\n22                                Lando Calrissian\n23                                       Boba Fett\n24                                           C-3P0\n25                                           R2 D2\n26                                   Jar Jar Binks\n27                                   Padme Amidala\n28                                            Yoda\n29                                                \n30                                                \n31                                                \n32                                                \n33                                                \n34                                                \n35                                                \n36                                                \n37                                                \nName: value, dtype: object\n0                                              NaN\n1                                                 \n2                                                 \n3         Star Wars: Episode I  The Phantom Menace\n4      Star Wars: Episode II  Attack of the Clones\n5      Star Wars: Episode III  Revenge of the Sith\n6                Star Wars: Episode IV  A New Hope\n7     Star Wars: Episode V The Empire Strikes Back\n8         Star Wars: Episode VI Return of the Jedi\n9         Star Wars: Episode I  The Phantom Menace\n10     Star Wars: Episode II  Attack of the Clones\n11     Star Wars: Episode III  Revenge of the Sith\n12               Star Wars: Episode IV  A New Hope\n13    Star Wars: Episode V The Empire Strikes Back\n14        Star Wars: Episode VI Return of the Jedi\n15                                        Han Solo\n16                                  Luke Skywalker\n17                            Princess Leia Organa\n18                                Anakin Skywalker\n19                                  Obi Wan Kenobi\n20                               Emperor Palpatine\n21                                     Darth Vader\n22                                Lando Calrissian\n23                                       Boba Fett\n24                                           C-3P0\n25                                           R2 D2\n26                                   Jar Jar Binks\n27                                   Padme Amidala\n28                                            Yoda\n29                                                \n30                                                \n31                                                \n32                                                \n33                                                \n34                                                \n35                                                \n36                                                \n37                                                \nName: value, dtype: object\n\n\n\n\nShow the code\n# %%\nprint(df_names.variable)\ndf_names.variable.str.strip().replace(variables_replace, regex=True)\nprint(df_names.variable)\n\n\n0                                          RespondentID\n1     Have you seen any of the 6 films in the Star W...\n2     Do you consider yourself to be a fan of the St...\n3     Which of the following Star Wars films have yo...\n4     Which of the following Star Wars films have yo...\n5     Which of the following Star Wars films have yo...\n6     Which of the following Star Wars films have yo...\n7     Which of the following Star Wars films have yo...\n8     Which of the following Star Wars films have yo...\n9     Please rank the Star Wars films in order of pr...\n10    Please rank the Star Wars films in order of pr...\n11    Please rank the Star Wars films in order of pr...\n12    Please rank the Star Wars films in order of pr...\n13    Please rank the Star Wars films in order of pr...\n14    Please rank the Star Wars films in order of pr...\n15    Please state whether you view the following ch...\n16    Please state whether you view the following ch...\n17    Please state whether you view the following ch...\n18    Please state whether you view the following ch...\n19    Please state whether you view the following ch...\n20    Please state whether you view the following ch...\n21    Please state whether you view the following ch...\n22    Please state whether you view the following ch...\n23    Please state whether you view the following ch...\n24    Please state whether you view the following ch...\n25    Please state whether you view the following ch...\n26    Please state whether you view the following ch...\n27    Please state whether you view the following ch...\n28    Please state whether you view the following ch...\n29                          Which character shot first?\n30         Are you familiar with the Expanded Universe?\n31    Do you consider yourself to be a fan of the Ex...\n32    Do you consider yourself to be a fan of the St...\n33                                               Gender\n34                                                  Age\n35                                     Household Income\n36                                            Education\n37                             Location (Census Region)\nName: variable, dtype: object\n0                                          RespondentID\n1     Have you seen any of the 6 films in the Star W...\n2     Do you consider yourself to be a fan of the St...\n3     Which of the following Star Wars films have yo...\n4     Which of the following Star Wars films have yo...\n5     Which of the following Star Wars films have yo...\n6     Which of the following Star Wars films have yo...\n7     Which of the following Star Wars films have yo...\n8     Which of the following Star Wars films have yo...\n9     Please rank the Star Wars films in order of pr...\n10    Please rank the Star Wars films in order of pr...\n11    Please rank the Star Wars films in order of pr...\n12    Please rank the Star Wars films in order of pr...\n13    Please rank the Star Wars films in order of pr...\n14    Please rank the Star Wars films in order of pr...\n15    Please state whether you view the following ch...\n16    Please state whether you view the following ch...\n17    Please state whether you view the following ch...\n18    Please state whether you view the following ch...\n19    Please state whether you view the following ch...\n20    Please state whether you view the following ch...\n21    Please state whether you view the following ch...\n22    Please state whether you view the following ch...\n23    Please state whether you view the following ch...\n24    Please state whether you view the following ch...\n25    Please state whether you view the following ch...\n26    Please state whether you view the following ch...\n27    Please state whether you view the following ch...\n28    Please state whether you view the following ch...\n29                          Which character shot first?\n30         Are you familiar with the Expanded Universe?\n31    Do you consider yourself to be a fan of the Ex...\n32    Do you consider yourself to be a fan of the St...\n33                                               Gender\n34                                                  Age\n35                                     Household Income\n36                                            Education\n37                             Location (Census Region)\nName: variable, dtype: object\n\n\n\n\nShow the code\n# %%\ndf_cols_use = (df_names\n    .assign(\n        value_replace = lambda x:  x.value.str.strip().replace(values_replace, regex=True),\n        variable_replace = lambda x: x.variable.str.strip().replace(variables_replace, regex=True)\n    )\n    .fillna(method = 'ffill')\n    .fillna(value = \"\")\n    .assign(column_names = lambda x: x.variable_replace.str.cat(x.value_replace, sep = \"__\").str.strip('__').str.lower())\n    )\ndf_cols_use\n\n\n\n\n\n\n\n\n\nvariable\nvalue\nclean_variable\nclean_value\ncolumn_name\nvalue_replace\nvariable_replace\ncolumn_names\n\n\n\n\n0\nRespondentID\n\nRespondentID\n\n\n\nRespondentID\nrespondentid\n\n\n1\nHave you seen any of the 6 films in the Star W...\n\nHave you seen any of the 6 films in the Star W...\n\nHave you seen any of the 6 films in the Star W...\n\nseen_any\nseen_any\n\n\n2\nDo you consider yourself to be a fan of the St...\n\nDo you consider yourself to be a fan of the St...\n\nDo you consider yourself to be a fan of the St...\n\nstar_wars_fans\nstar_wars_fans\n\n\n3\nWhich of the following Star Wars films have yo...\nStar Wars: Episode I The Phantom Menace\nseen\nStar Wars: Episode I The Phantom Menace\nseen__Star Wars: Episode I The Phantom Menace\nI__The_Phantom_Menace\nseen\nseen__i__the_phantom_menace\n\n\n4\nWhich of the following Star Wars films have yo...\nStar Wars: Episode II Attack of the Clones\nseen\nStar Wars: Episode II Attack of the Clones\nseen__Star Wars: Episode II Attack of the Clones\nII__Attack_of_the_Clones\nseen\nseen__ii__attack_of_the_clones\n\n\n5\nWhich of the following Star Wars films have yo...\nStar Wars: Episode III Revenge of the Sith\nseen\nStar Wars: Episode III Revenge of the Sith\nseen__Star Wars: Episode III Revenge of the Sith\nIII__Revenge_of_the_Sith\nseen\nseen__iii__revenge_of_the_sith\n\n\n6\nWhich of the following Star Wars films have yo...\nStar Wars: Episode IV A New Hope\nseen\nStar Wars: Episode IV A New Hope\nseen__Star Wars: Episode IV A New Hope\nIV__A_New_Hope\nseen\nseen__iv__a_new_hope\n\n\n7\nWhich of the following Star Wars films have yo...\nStar Wars: Episode V The Empire Strikes Back\nseen\nStar Wars: Episode V The Empire Strikes Back\nseen__Star Wars: Episode V The Empire Strikes ...\nV_The_Empire_Strikes_Back\nseen\nseen__v_the_empire_strikes_back\n\n\n8\nWhich of the following Star Wars films have yo...\nStar Wars: Episode VI Return of the Jedi\nseen\nStar Wars: Episode VI Return of the Jedi\nseen__Star Wars: Episode VI Return of the Jedi\nVI_Return_of_the_Jedi\nseen\nseen__vi_return_of_the_jedi\n\n\n9\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode I The Phantom Menace\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode I The Phantom Menace\nPlease rank the Star Wars films in order of pr...\nI__The_Phantom_Menace\nrank\nrank__i__the_phantom_menace\n\n\n10\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode II Attack of the Clones\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode II Attack of the Clones\nPlease rank the Star Wars films in order of pr...\nII__Attack_of_the_Clones\nrank\nrank__ii__attack_of_the_clones\n\n\n11\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode III Revenge of the Sith\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode III Revenge of the Sith\nPlease rank the Star Wars films in order of pr...\nIII__Revenge_of_the_Sith\nrank\nrank__iii__revenge_of_the_sith\n\n\n12\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode IV A New Hope\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode IV A New Hope\nPlease rank the Star Wars films in order of pr...\nIV__A_New_Hope\nrank\nrank__iv__a_new_hope\n\n\n13\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode V The Empire Strikes Back\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode V The Empire Strikes Back\nPlease rank the Star Wars films in order of pr...\nV_The_Empire_Strikes_Back\nrank\nrank__v_the_empire_strikes_back\n\n\n14\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode VI Return of the Jedi\nPlease rank the Star Wars films in order of pr...\nStar Wars: Episode VI Return of the Jedi\nPlease rank the Star Wars films in order of pr...\nVI_Return_of_the_Jedi\nrank\nrank__vi_return_of_the_jedi\n\n\n15\nPlease state whether you view the following ch...\nHan Solo\nPlease state whether you view the following ch...\nHan Solo\nPlease state whether you view the following ch...\nHan_Solo\nview\nview__han_solo\n\n\n16\nPlease state whether you view the following ch...\nLuke Skywalker\nPlease state whether you view the following ch...\nLuke Skywalker\nPlease state whether you view the following ch...\nLuke_Skywalker\nview\nview__luke_skywalker\n\n\n17\nPlease state whether you view the following ch...\nPrincess Leia Organa\nPlease state whether you view the following ch...\nPrincess Leia Organa\nPlease state whether you view the following ch...\nPrincess_Leia_Organa\nview\nview__princess_leia_organa\n\n\n18\nPlease state whether you view the following ch...\nAnakin Skywalker\nPlease state whether you view the following ch...\nAnakin Skywalker\nPlease state whether you view the following ch...\nAnakin_Skywalker\nview\nview__anakin_skywalker\n\n\n19\nPlease state whether you view the following ch...\nObi Wan Kenobi\nPlease state whether you view the following ch...\nObi Wan Kenobi\nPlease state whether you view the following ch...\nObi_Wan_Kenobi\nview\nview__obi_wan_kenobi\n\n\n20\nPlease state whether you view the following ch...\nEmperor Palpatine\nPlease state whether you view the following ch...\nEmperor Palpatine\nPlease state whether you view the following ch...\nEmperor_Palpatine\nview\nview__emperor_palpatine\n\n\n21\nPlease state whether you view the following ch...\nDarth Vader\nPlease state whether you view the following ch...\nDarth Vader\nPlease state whether you view the following ch...\nDarth_Vader\nview\nview__darth_vader\n\n\n22\nPlease state whether you view the following ch...\nLando Calrissian\nPlease state whether you view the following ch...\nLando Calrissian\nPlease state whether you view the following ch...\nLando_Calrissian\nview\nview__lando_calrissian\n\n\n23\nPlease state whether you view the following ch...\nBoba Fett\nPlease state whether you view the following ch...\nBoba Fett\nPlease state whether you view the following ch...\nBoba_Fett\nview\nview__boba_fett\n\n\n24\nPlease state whether you view the following ch...\nC-3P0\nPlease state whether you view the following ch...\nC-3P0\nPlease state whether you view the following ch...\nC-3P0\nview\nview__c-3p0\n\n\n25\nPlease state whether you view the following ch...\nR2 D2\nPlease state whether you view the following ch...\nR2 D2\nPlease state whether you view the following ch...\nR2_D2\nview\nview__r2_d2\n\n\n26\nPlease state whether you view the following ch...\nJar Jar Binks\nPlease state whether you view the following ch...\nJar Jar Binks\nPlease state whether you view the following ch...\nJar_Jar_Binks\nview\nview__jar_jar_binks\n\n\n27\nPlease state whether you view the following ch...\nPadme Amidala\nPlease state whether you view the following ch...\nPadme Amidala\nPlease state whether you view the following ch...\nPadme_Amidala\nview\nview__padme_amidala\n\n\n28\nPlease state whether you view the following ch...\nYoda\nPlease state whether you view the following ch...\nYoda\nPlease state whether you view the following ch...\nYoda\nview\nview__yoda\n\n\n29\nWhich character shot first?\n\nWhich character shot first?\n\nWhich character shot first?__\n\nshot_first\nshot_first\n\n\n30\nAre you familiar with the Expanded Universe?\n\nAre you familiar with the Expanded Universe?\n\nAre you familiar with the Expanded Universe?__\n\nknow_expanded\nknow_expanded\n\n\n31\nDo you consider yourself to be a fan of the Ex...\n\nDo you consider yourself to be a fan of the Ex...\n\nDo you consider yourself to be a fan of the Ex...\n\nDo_you_consider_yourself_to_be_a_fan_of_the_Ex...\ndo_you_consider_yourself_to_be_a_fan_of_the_ex...\n\n\n32\nDo you consider yourself to be a fan of the St...\n\nDo you consider yourself to be a fan of the St...\n\nDo you consider yourself to be a fan of the St...\n\nstar_trek_fan\nstar_trek_fan\n\n\n33\nGender\n\nGender\n\nGender__\n\nGender\ngender\n\n\n34\nAge\n\nAge\n\nAge__\n\nAge\nage\n\n\n35\nHousehold Income\n\nHousehold Income\n\nHousehold Income__\n\nHousehold_Income\nhousehold_income\n\n\n36\nEducation\n\nEducation\n\nEducation__\n\nEducation\neducation\n\n\n37\nLocation (Census Region)\n\nLocation (Census Region)\n\nLocation (Census Region)__\n\nLocation_(Census_Region)\nlocation_(census_region)\n\n\n\n\n\n\n\n\n\nShow the code\n#replace the colums with the new column names\ndf.columns = df_cols_use.column_names\ndf.head()\n\n\n\n\n\n\n\n\ncolumn_names\nrespondentid\nseen_any\nstar_wars_fans\nseen__i__the_phantom_menace\nseen__ii__attack_of_the_clones\nseen__iii__revenge_of_the_sith\nseen__iv__a_new_hope\nseen__v_the_empire_strikes_back\nseen__vi_return_of_the_jedi\nrank__i__the_phantom_menace\n...\nview__yoda\nshot_first\nknow_expanded\ndo_you_consider_yourself_to_be_a_fan_of_the_expanded_universe?Œæ\nstar_trek_fan\ngender\nage\nhousehold_income\neducation\nlocation_(census_region)\n\n\n\n\n0\n3292879998\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n3.0\n...\nVery favorably\nI don't understand this question\nYes\nNo\nNo\nMale\n18-29\nNaN\nHigh school degree\nSouth Atlantic\n\n\n1\n3292879538\nNo\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nYes\nMale\n18-29\n$0 - $24,999\nBachelor degree\nWest South Central\n\n\n2\n3292765271\nYes\nNo\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nNaN\nNaN\nNaN\n1.0\n...\nUnfamiliar (N/A)\nI don't understand this question\nNo\nNaN\nNo\nMale\n18-29\n$0 - $24,999\nHigh school degree\nWest North Central\n\n\n3\n3292763116\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n5.0\n...\nVery favorably\nI don't understand this question\nNo\nNaN\nYes\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n4\n3292731220\nYes\nYes\nStar Wars: Episode I The Phantom Menace\nStar Wars: Episode II Attack of the Clones\nStar Wars: Episode III Revenge of the Sith\nStar Wars: Episode IV A New Hope\nStar Wars: Episode V The Empire Strikes Back\nStar Wars: Episode VI Return of the Jedi\n5.0\n...\nSomewhat favorably\nGreedo\nYes\nNo\nNo\nMale\n18-29\n$100,000 - $149,999\nSome college or Associate degree\nWest North Central\n\n\n\n\n5 rows × 38 columns\n\n\n\n\n\nShow the code\n#new_age column to replace age then do an age max/min scalar, standard\n###This is too see what the model actually likes. \n#new_income to replace household_income\n#SClearn Normalizers scalar",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-2",
    "href": "Projects/project5.html#questiontask-2",
    "title": "Client Report - [Star Wars comprehension]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nClean and format the data so that it can be used in a machine learning model. As you format the data, you should complete each item listed below. In your final report provide example(s) of the reformatted data with a short description of the changes made. Filter the dataset to respondents that have seen at least one film. Create a new column that converts the age ranges to a single number. Drop the age range categorical column. Create a new column that converts the education groupings to a single number. Drop the school categorical column Create a new column that converts the income ranges to a single number. Drop the income range categorical column. Create your target (also known as “y” or “label”) column based on the new income range column. One-hot encode all remaining categorical columns.\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nplot example\n# Include and execute your code here\n# chart = px.bar(df.head(200),\n#     x=\"name\", \n#     y=\"AK\"\n# )\n# chart.show()\n\n\n\n\ntable example\n# Include and execute your code here\n# mydat = df.head(1000)\\\n#     .groupby('year')\\\n#     .sum()\\\n#     .reset_index()\\\n#     .tail(10)\\\n#     .filter([\"year\", \"AK\",\"AR\"])\n\n# display(mydat)",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-3",
    "href": "Projects/project5.html#questiontask-3",
    "title": "Client Report - [Star Wars comprehension]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nCValidate that the data provided on GitHub lines up with the article by recreating 2 of the visuals from the article.\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n\n\nplot example\n# Include and execute your code here\n# chart = px.bar(df.head(200),\n#     x=\"name\", \n#     y=\"AK\"\n# )\n# chart.show()\n\n\n\n\ntable example\n# Include and execute your code here\n# mydat = df.head(1000)\\\n#     .groupby('year')\\\n#     .sum()\\\n#     .reset_index()\\\n#     .tail(10)\\\n#     .filter([\"year\", \"AK\",\"AR\"])\n\n# display(mydat)",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask4",
    "href": "Projects/project5.html#questiontask4",
    "title": "Client Report - [Star Wars comprehension]",
    "section": "QUESTION|TASK4",
    "text": "QUESTION|TASK4\nBuild a machine learning model that predicts whether a person makes more than $50k. Describe your model and report the accuracy.",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Darren Chambers CV",
    "section": "",
    "text": "Computer Science, Cyber Security, French\n\nMy TryHackMe page | My Linkedin\n\n\n\nStudying Computer Science, Ethical Hacker, Japanese, R\n\n\nCyber Security, Computer Science, French\n\n\n\nCyber Security, Computer Hardware, drones\n\n\n\n\n2023-2024 Brigham YOung University- Idaho 2023-2024 Cisco Skills For All\n\n\n\n\n\n\n2020-2023 United States Army, North Carolina\n2019 Sales Manager,"
  },
  {
    "objectID": "resume.html#currently",
    "href": "resume.html#currently",
    "title": "Darren Chambers CV",
    "section": "",
    "text": "Studying Computer Science, Ethical Hacker, Japanese, R\n\n\nCyber Security, Computer Science, French\n\n\n\nCyber Security, Computer Hardware, drones"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Darren Chambers CV",
    "section": "",
    "text": "2023-2024 Brigham YOung University- Idaho 2023-2024 Cisco Skills For All"
  },
  {
    "objectID": "resume.html#occupation",
    "href": "resume.html#occupation",
    "title": "Darren Chambers CV",
    "section": "",
    "text": "2020-2023 United States Army, North Carolina\n2019 Sales Manager,"
  }
]