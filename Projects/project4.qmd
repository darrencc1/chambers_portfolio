---
title: "Client Report - [Prediction of Houses Built Before 1980 in Colorado]"
subtitle: "Course DS 250"
author: "[Darren Chambers]"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---

```{python}
#| label: libraries
#| include: false
import pandas as pd
import numpy as np
import plotly.express as px

```

```{python}
from sklearn.model_selection import train_test_split
from sklearn import tree 
from sklearn.metrics import RocCurveDisplay
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

```



## Elevator pitch

_paste your elevator pitch here_
_A SHORT (4-5 SENTENCES) PARAGRAPH THAT `DESCRIBES KEY INSIGHTS` TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS._

```{python}
#| label: project data
#| code-summary: Read and format project data
# Include and execute your code here
df = pd.read_csv("https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv")

dwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')

dwellings_ml.head()
```


__Highlight the Questions and Tasks__

## QUESTION|TASK 1

__Create 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.__

_Looking at the chart we see that almost 25% was from one-story buildings. After this the effectivness of the potential relations ships drop drastically. the top 5 variables make up ~61% of the important data. It could be helpful to not include all of hte potential relationships and only keep the top 10 (78%) or top 5 (61%)_

```{python}
X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)
y_pred = dwellings_ml.filter(regex = "before1980")
X_train, X_test, y_train, y_test = train_test_split(
    X_pred, y_pred, test_size = .36, random_state = 66)

    #70, .30, 66, .34, 80,.20
```

```{python}

```

```{python}

   

    #70, .30, 66, .34, 80,.20
    #     x='parcel',
#     y='abstrpod',
#     color='yrbuilt',
#     title='Does age affect whether a passenger survived?',
#     # marginal="box", # or violin, rug
#     # hover_data=
# )
clf = RandomForestClassifier() #Rand Forest is similar, try it. 
#linear regression, 
clf = clf.fit(X_train, y_train)
# Fit: fit the model to said data
y_pred = clf.predict(X_test)
y_probs = clf.predict_proba(X_test)
y_pred = clf.predict(X_test)
```

```{python}
accuracy_score(y_test, y_pred)
```

_include figures in chunks and discuss your findings in the figure._

```{python}

# clf is a classifier.
clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. 
#linear regression, 
clf = clf.fit(X_train, y_train)
# Fit: fit the model to said data
y_pred = clf.predict(X_test)
y_probs = clf.predict_proba(X_test)

# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)
```

```{python}
# %% this is the y_pred and y_probes from cell above
print(metrics.classification_report(y_pred, y_test))
accuracy_score(y_test, y_pred)
# Decision matrix 0 and 1. 
```

```{python}
df_features = pd.DataFrame(
    {'f_names': X_train.columns, 
    'f_values': clf.feature_importances_}).sort_values('f_values', ascending = False)
```


```{python}
#%%
chart = px.bar(df_features.head(10), #You can change this 10 based on top features you want to see.
    x='f_values', 
    y='f_names'
)

chart.update_layout(yaxis={'categoryorder':'total ascending'})
# 0-1 is based on % 1 is 100% 
# f(x) = y (y is target, does not change here)
# f is algorithm, Try 4
# (x) is features 
```

```{python}
# %% 
#roc curve
# metrics.RocCurveDisplay.from_estimator(clf, X_test, y_test)
```

## QUESTION|TASK 2

__Build a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.__

_Below I will show you some different results based on tst size as well as how RandomForest Algorithm vs Decision Tree algorithm._

```{python}

X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)
y_pred = dwellings_ml.filter(regex = "before1980")
X_train, X_test, y_train, y_test = train_test_split(
    X_pred, y_pred, test_size = .34, random_state = 66)

    #70, .30, 66, .34, 80,.20
```

```{python}  

    #70, .30, 66, .34, 80,.20
    #     x='parcel',
#     y='abstrpod',
#     color='yrbuilt',
#     title='Does age affect whether a passenger survived?',
#     # marginal="box", # or violin, rug
#     # hover_data=
# )
clf = RandomForestClassifier() #Rand Forest is similar, try it. 
#linear regression, 
clf = clf.fit(X_train, y_train)
# Fit: fit the model to said data
y_pred = clf.predict(X_test)
y_probs = clf.predict_proba(X_test)
y_pred = clf.predict(X_test)
print(metrics.classification_report(y_pred, y_test))
accuracy_score(y_test, y_pred)
```

```{python}


# clf is a classifier.
clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. 
#linear regression, 
clf = clf.fit(X_train, y_train)
# Fit: fit the model to said data
y_pred = clf.predict(X_test)
y_probs = clf.predict_proba(X_test)
# %% this is the y_pred and y_probes from cell above
print(metrics.classification_report(y_pred, y_test))
accuracy_score(y_test, y_pred)
# Decision matrix 0 and 1. 

# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)
```

_This next set of data shows how changing the test from 34|66 to 30|70 affected the accuracy. In both cases the accuracy dropped by .4 or .3 % dropping the Decision Tree Algorithm below the 90% accuracy requirement._

```{python}

X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)
y_pred = dwellings_ml.filter(regex = "before1980")
X_train, X_test, y_train, y_test = train_test_split(
    X_pred, y_pred, test_size = .30, random_state = 70)
    #70, .30, 66, .34, 80,.20
```

```{python}  

    #70, .30, 66, .34, 80,.20
    #     x='parcel',
#     y='abstrpod',
#     color='yrbuilt',
#     title='Does age affect whether a passenger survived?',
#     # marginal="box", # or violin, rug
#     # hover_data=
# )
clf = RandomForestClassifier() #Rand Forest is similar, try it. 
#linear regression, 
clf = clf.fit(X_train, y_train)
# Fit: fit the model to said data
y_pred = clf.predict(X_test)
y_probs = clf.predict_proba(X_test)
y_pred = clf.predict(X_test)
print(metrics.classification_report(y_pred, y_test))
accuracy_score(y_test, y_pred)
```

```{python}


# clf is a classifier.
clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. 
#linear regression, 
clf = clf.fit(X_train, y_train)
# Fit: fit the model to said data
y_pred = clf.predict(X_test)
y_probs = clf.predict_proba(X_test)
# %% this is the y_pred and y_probes from cell above
print(metrics.classification_report(y_pred, y_test))
accuracy_score(y_test, y_pred)
# Decision matrix 0 and 1. 

# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)
```

_For this set I am using the 80/20 test and with this the accuracy went back up reaching 92.45% for the random Tree algorithm which is the highest of all the tests so far._
```{python}

X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)
y_pred = dwellings_ml.filter(regex = "before1980")
X_train, X_test, y_train, y_test = train_test_split(
    X_pred, y_pred, test_size = .2, random_state = 80)
    #70, .30, 66, .34, 80,.20
```

```{python}  

    #70, .30, 66, .34, 80,.20
    #     x='parcel',
#     y='abstrpod',
#     color='yrbuilt',
#     title='Does age affect whether a passenger survived?',
#     # marginal="box", # or violin, rug
#     # hover_data=
# )
clf = RandomForestClassifier() #Rand Forest is similar, try it. 
#linear regression, 
clf = clf.fit(X_train, y_train)
# Fit: fit the model to said data
y_pred = clf.predict(X_test)
y_probs = clf.predict_proba(X_test)
y_pred = clf.predict(X_test)
print(metrics.classification_report(y_pred, y_test))
accuracy_score(y_test, y_pred)
```

```{python}


# clf is a classifier.
clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. 
#linear regression, 
clf = clf.fit(X_train, y_train)
# Fit: fit the model to said data
y_pred = clf.predict(X_test)
y_probs = clf.predict_proba(X_test)
# %% this is the y_pred and y_probes from cell above
print(metrics.classification_report(y_pred, y_test))
accuracy_score(y_test, y_pred)
# Decision matrix 0 and 1. 

# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)
```





## QUESTION|TASK 3

__Justify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.__

_type your results and analysis here_

```{python}
#| label: Q3
#| code-summary: Read and format data
# Include and execute your code here


```

_include figures in chunks and discuss your findings in the figure._

```{python}
#| label: Q3 chart
#| code-summary: plot example
#| fig-cap: "My useless chart"
#| fig-align: center
# Include and execute your code here
chart = px.bar(df.head(200),
    x="name", 
    y="AK"
)
chart.show()
```


```{python}
#| label: Q3 table
#| code-summary: table example
#| tbl-cap: "Not much of a table"
#| tbl-cap-location: top
# Include and execute your code here
mydat = df.head(1000)\
    .groupby('year')\
    .sum()\
    .reset_index()\
    .tail(10)\
    .filter(["year", "AK","AR"])

display(mydat)

```

## Question|Task 4
__Describe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.__