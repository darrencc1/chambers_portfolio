{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Client Report - [Prediction of Houses Built Before 1980 in Colorado]\"\n",
        "subtitle: \"Course DS 250\"\n",
        "author: \"[Darren Chambers]\"\n",
        "format:\n",
        "  html:\n",
        "    self-contained: true\n",
        "    page-layout: full\n",
        "    title-block-banner: true\n",
        "    toc: true\n",
        "    toc-depth: 3\n",
        "    toc-location: body\n",
        "    number-sections: false\n",
        "    html-math-method: katex\n",
        "    code-fold: true\n",
        "    code-summary: \"Show the code\"\n",
        "    code-overflow: wrap\n",
        "    code-copy: hover\n",
        "    code-tools:\n",
        "        source: false\n",
        "        toggle: true\n",
        "        caption: See code\n",
        "execute: \n",
        "  warning: false\n",
        "    \n",
        "---"
      ],
      "id": "b83f907e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: libraries\n",
        "#| include: false\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n"
      ],
      "id": "libraries",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import tree \n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n"
      ],
      "id": "5ee0b330",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Elevator pitch\n",
        "\n",
        "_paste your elevator pitch here_\n",
        "_A SHORT (4-5 SENTENCES) PARAGRAPH THAT `DESCRIBES KEY INSIGHTS` TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS._\n"
      ],
      "id": "76685280"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: project data\n",
        "#| code-summary: Read and format project data\n",
        "# Include and execute your code here\n",
        "df = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n",
        "\n",
        "dwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')\n",
        "\n",
        "df2 = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_neighborhoods_ml/dwellings_neighborhoods_ml.csv\")\n",
        "dwellings_ml.head()"
      ],
      "id": "project-data",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Highlight the Questions and Tasks__\n",
        "\n",
        "## QUESTION|TASK 1\n",
        "\n",
        "__Create 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.__\n",
        "\n",
        "_There are many factors that we can use to help our algorithms learn and predict homes that were made before 1980._\n"
      ],
      "id": "d6606ed4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .34, random_state = 66)\n",
        "\n",
        "    #70, .30, 66, .34, 80,.20"
      ],
      "id": "fb343431",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "    #70, .30, 66, .34, 80,.20\n",
        "    #     x='parcel',\n",
        "#     y='abstrpod',\n",
        "#     color='yrbuilt',\n",
        "#     title='Does age affect whether a passenger survived?',\n",
        "#     # marginal=\"box\", # or violin, rug\n",
        "#     # hover_data=\n",
        "# )\n",
        "clf = RandomForestClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "id": "8fd801e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_Looking at the chart we see that almost 25% was from one-story buildings. After this the effectivness of the potential relationships drop drastically. So figuring out which data is almost irrelevant, in this case meaning well below 1% helpts my algorithms learn and predict houses made before 1980._\n"
      ],
      "id": "29ce59a9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# clf is a classifier.\n",
        "clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "\n",
        "# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)"
      ],
      "id": "415827ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_features = pd.DataFrame(\n",
        "    {'f_names': X_train.columns, \n",
        "    'f_values': clf.feature_importances_}).sort_values('f_values', ascending = False)"
      ],
      "id": "717efdcf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#%%\n",
        "chart = px.bar(df_features.head(20), #You can change this 10 based on top features you want to see.\n",
        "    x='f_values', \n",
        "    y='f_names'\n",
        ")\n",
        "\n",
        "chart.update_layout(yaxis={'categoryorder':'total ascending'})\n",
        "# 0-1 is based on % 1 is 100% \n",
        "# f(x) = y (y is target, does not change here)\n",
        "# f is algorithm, Try 4\n",
        "# (x) is features "
      ],
      "id": "7739f7bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## QUESTION|TASK 2\n",
        "\n",
        "__Build a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.__\n",
        "\n",
        "_Below I will show you some different results based on test size as well as\n",
        "RandomForest Algorithm vs Decision Tree algorithm. This first test size is 34 and random state 66._\n"
      ],
      "id": "7a58c202"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .34, random_state = 66)\n",
        "\n",
        "    # Extract the top 10 feature names\n",
        "\n",
        "\n",
        "X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .34, random_state = 66)\n",
        "\n",
        "\n",
        "    #70, .30, 66, .34, 80,.20"
      ],
      "id": "b09c20a4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# clf_filtered = RandomForestClassifier()\n",
        "# clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())"
      ],
      "id": "5f94fc55",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assuming df_features is already created and sorted by feature importance\n",
        "top_20_features = df_features.head(20)['f_names'].tolist()\n",
        "\n",
        "# Now, filter X_train and X_test to include only the top 10 features\n",
        "X_train_filtered = X_train[top_20_features]\n",
        "X_test_filtered = X_test[top_20_features]\n",
        "\n",
        "# Training your model using these filtered datasets\n",
        "clf_filtered = RandomForestClassifier()\n",
        "clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n",
        "\n",
        "# Making predictions with the test set filtered to top 10 features\n",
        "y_pred_filtered = clf_filtered.predict(X_test_filtered)\n",
        "\n",
        "clf = RandomForestClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "# Evaluating the model with the accuracy score using Top 10 Features\n",
        "accuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\n",
        "print(\"RandomForest Accuracy with Top 20 Features 34/66:\", accuracy_score_filtered)"
      ],
      "id": "3d9f1cf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .34, random_state = 66)"
      ],
      "id": "3336c801",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_20_features = df_features.head(20)['f_names'].tolist()\n",
        "\n",
        "# Now, filter X_train and X_test to include only the top 10 features\n",
        "X_train_filtered = X_train[top_20_features]\n",
        "X_test_filtered = X_test[top_20_features]\n",
        "\n",
        "# Training your model using these filtered datasets\n",
        "clf_filtered = tree.DecisionTreeClassifier()\n",
        "clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n",
        "\n",
        "# Making predictions with the test set filtered to top 10 features\n",
        "y_pred_filtered = clf_filtered.predict(X_test_filtered)\n",
        "# clf is a classifier.\n",
        "clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "print(\"Decision Tree Accuracy with Top 20 Features 34/66:\", accuracy_score_filtered)\n",
        "# Decision matrix 0 and 1. \n",
        "\n",
        "# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)"
      ],
      "id": "7896ccba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_This next set of data shows how changing the test from 34|66 to 30|70 affected the accuracy. In both cases the accuracy dropped by .2 - .4% depending on the learning._\n"
      ],
      "id": "e3b33ae8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .30, random_state = 70)\n",
        "    #70, .30, 66, .34, 80,.20"
      ],
      "id": "559e73c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assuming df_features is already created and sorted by feature importance\n",
        "top_20_features = df_features.head(20)['f_names'].tolist()\n",
        "\n",
        "# Now, filter X_train and X_test to include only the top 10 features\n",
        "X_train_filtered = X_train[top_20_features]\n",
        "X_test_filtered = X_test[top_20_features]\n",
        "\n",
        "# Training your model using these filtered datasets\n",
        "clf_filtered = RandomForestClassifier()\n",
        "clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n",
        "\n",
        "# Making predictions with the test set filtered to top 10 features\n",
        "y_pred_filtered = clf_filtered.predict(X_test_filtered)\n",
        "\n",
        "clf = RandomForestClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "# Evaluating the model with the accuracy score using Top 10 Features\n",
        "accuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\n",
        "print(\"RandomForest Accuracy with Top 20 Features 30/70:\", accuracy_score_filtered)"
      ],
      "id": "9f359f9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .30, random_state = 70)\n",
        "    #70, .30, 66, .34, 80,.20"
      ],
      "id": "2b1993ce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_20_features = df_features.head(20)['f_names'].tolist()\n",
        "\n",
        "# Now, filter X_train and X_test to include only the top 10 features\n",
        "X_train_filtered = X_train[top_20_features]\n",
        "X_test_filtered = X_test[top_20_features]\n",
        "\n",
        "# Training your model using these filtered datasets\n",
        "clf_filtered = tree.DecisionTreeClassifier()\n",
        "clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n",
        "\n",
        "# Making predictions with the test set filtered to top 10 features\n",
        "y_pred_filtered = clf_filtered.predict(X_test_filtered)\n",
        "# clf is a classifier.\n",
        "clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "print(\"Decision Tree Accuracy with Top 20 Features 30/70:\", accuracy_score_filtered)\n",
        "# Decision matrix 0 and 1. \n",
        "\n",
        "# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)"
      ],
      "id": "a1c3e92d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_For this set I am using the 80/20 test and with this the accuracy went back up reaching 92.45% this dropped to 92.34% When using only the top 10 and up to 92.77 if I used the top 20 for the random Tree algorithm which is the highest of all the tests so far._\n"
      ],
      "id": "30cc0a36"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .2, random_state = 80)\n",
        "    #70, .30, 66, .34, 80,.20"
      ],
      "id": "b3643cb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_20_features = df_features.head(20)['f_names'].tolist()\n",
        "\n",
        "# Now, filter X_train and X_test to include only the top 10 features\n",
        "X_train_filtered = X_train[top_20_features]\n",
        "X_test_filtered = X_test[top_20_features]\n",
        "\n",
        "# Training your model using these filtered datasets\n",
        "clf_filtered = tree.DecisionTreeClassifier()\n",
        "clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n",
        "\n",
        "# Making predictions with the test set filtered to top 10 features\n",
        "y_pred_filtered = clf_filtered.predict(X_test_filtered)\n",
        "# clf is a classifier.\n",
        "clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "print(\"Decision Tree Accuracy with Top 20 Features 20/80:\", accuracy_score_filtered)"
      ],
      "id": "f232cc40",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "    #70, .30, 66, .34, 80,.20\n",
        "    #     x='parcel',\n",
        "#     y='abstrpod',\n",
        "#     color='yrbuilt',\n",
        "#     title='Does age affect whether a passenger survived?',\n",
        "#     # marginal=\"box\", # or violin, rug\n",
        "#     # hover_data=\n",
        "# )\n",
        "clf = RandomForestClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "id": "6863becd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# clf is a classifier.\n",
        "clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "# Decision matrix 0 and 1. \n",
        "\n",
        "# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)"
      ],
      "id": "b404c2d8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# clf is a classifier.\n",
        "clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "# Decision matrix 0 and 1. \n",
        "\n",
        "# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)"
      ],
      "id": "71b52c96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.join(df2, lsuffix=\"parcel\", rsuffix=\"parcel\")\n",
        "# print(\"This is the joined dataframe of Neighborhood dwellings and dwellings ml. \")"
      ],
      "id": "cbb4ce3c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"This is the joined dataframe of Neighborhood dwellings and dwellings ml. \")"
      ],
      "id": "430fc1f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .34, random_state = 66)\n",
        "\n",
        "    # Extract the top 10 feature names\n",
        "\n",
        "\n",
        "# X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "# y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     X_pred, y_pred, test_size = .34, random_state = 66)\n",
        "\n",
        "\n",
        "    #70, .30, 66, .34, 80,.20"
      ],
      "id": "d6872dab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clf_filtered = RandomForestClassifier()\n",
        "clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel()) "
      ],
      "id": "c5a605c6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assuming df_features is already created and sorted by feature importance\n",
        "top_20_features = df_features.head(10)['f_names'].tolist()\n",
        "\n",
        "# Now, filter X_train and X_test to include only the top 10 features\n",
        "X_train_filtered = X_train[top_20_features]\n",
        "X_test_filtered = X_test[top_20_features]\n",
        "\n",
        "# Training your model using these filtered datasets\n",
        "clf_filtered = RandomForestClassifier()\n",
        "clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n",
        "\n",
        "# Making predictions with the test set filtered to top 10 features\n",
        "y_pred_filtered = clf_filtered.predict(X_test_filtered)\n",
        "\n",
        "clf = RandomForestClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "# Evaluating the model with the accuracy score using Top 10 Features\n",
        "accuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\n",
        "print(\"RandomForest Accuracy with Top 20 Features, 34/66 and joined with neighborhood dwellings  (df2):\", accuracy_score_filtered)"
      ],
      "id": "c48f8f0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .34, random_state = 66)"
      ],
      "id": "dd564bf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clf_filtered = tree.DecisionTreeClassifier()\n",
        "clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel()) \n",
        "# print(metrics.classification_report(y_pred, y_test))\n",
        "# accuracy_score(y_test, y_pred)"
      ],
      "id": "962c5fbe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .34, random_state = 66)"
      ],
      "id": "180e3f5a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_20_features = df_features.head(10)['f_names'].tolist()\n",
        "\n",
        "# Now, filter X_train and X_test to include only the top 10 features\n",
        "X_train_filtered = X_train[top_20_features]\n",
        "X_test_filtered = X_test[top_20_features]\n",
        "\n",
        "# Training your model using these filtered datasets\n",
        "# clf_filtered = tree.DecisionTreeClassifier()\n",
        "# clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n",
        "\n",
        "# Making predictions with the test set filtered to top 10 features\n",
        "y_pred_filtered = clf_filtered.predict(X_test_filtered)\n",
        "# clf is a classifier.\n",
        "clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "print(\"Decision Tree Accuracy with Top 20 Features 34/66:\", accuracy_score_filtered)\n",
        "# Decision matrix 0 and 1. \n",
        "\n",
        "# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)"
      ],
      "id": "7a95814d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_This next set of data shows how changing the test from 34|66 to 30|70 affected the accuracy. In both cases the accuracy dropped by .2 - .4% depending on the learning._\n"
      ],
      "id": "a2d6f42a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .30, random_state = 70)\n",
        "    #70, .30, 66, .34, 80,.20"
      ],
      "id": "5d7d5152",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assuming df_features is already created and sorted by feature importance\n",
        "top_20_features = df_features.head(20)['f_names'].tolist()\n",
        "\n",
        "# Now, filter X_train and X_test to include only the top 10 features\n",
        "X_train_filtered = X_train[top_20_features]\n",
        "X_test_filtered = X_test[top_20_features]\n",
        "\n",
        "# Training your model using these filtered datasets\n",
        "clf_filtered = RandomForestClassifier()\n",
        "clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n",
        "\n",
        "# Making predictions with the test set filtered to top 10 features\n",
        "y_pred_filtered = clf_filtered.predict(X_test_filtered)\n",
        "\n",
        "clf = RandomForestClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "# Evaluating the model with the accuracy score using Top 10 Features\n",
        "accuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\n",
        "print(\"RandomForest Accuracy with Top 20 Features 30/70:\", accuracy_score_filtered)"
      ],
      "id": "699bbf41",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .30, random_state = 70)\n",
        "    #70, .30, 66, .34, 80,.20"
      ],
      "id": "1b0ff7ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_20_features = df_features.head(20)['f_names'].tolist()\n",
        "\n",
        "# Now, filter X_train and X_test to include only the top 10 features\n",
        "X_train_filtered = X_train[top_20_features]\n",
        "X_test_filtered = X_test[top_20_features]\n",
        "\n",
        "# Training your model using these filtered datasets\n",
        "clf_filtered = tree.DecisionTreeClassifier()\n",
        "clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n",
        "\n",
        "# Making predictions with the test set filtered to top 10 features\n",
        "y_pred_filtered = clf_filtered.predict(X_test_filtered)\n",
        "# clf is a classifier.\n",
        "clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "print(\"Decision Tree Accuracy with Top 20 Features 30/70:\", accuracy_score_filtered)\n",
        "# Decision matrix 0 and 1. \n",
        "\n",
        "# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)"
      ],
      "id": "2dc549eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "_For this set I am using the 80/20 test and with this the accuracy went back up reaching 92.45% this dropped to 92.34% When using only the top 10 and up to 92.77 if I used the top 20 for the random Tree algorithm which is the highest of all the tests so far._\n"
      ],
      "id": "8a0817b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n",
        "y_pred = dwellings_ml.filter(regex = \"before1980\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pred, y_pred, test_size = .2, random_state = 80)\n",
        "    #70, .30, 66, .34, 80,.20"
      ],
      "id": "8b314b8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "top_20_features = df_features.head(20)['f_names'].tolist()\n",
        "\n",
        "# Now, filter X_train and X_test to include only the top 10 features\n",
        "X_train_filtered = X_train[top_20_features]\n",
        "X_test_filtered = X_test[top_20_features]\n",
        "\n",
        "# Training your model using these filtered datasets\n",
        "clf_filtered = tree.DecisionTreeClassifier()\n",
        "clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n",
        "\n",
        "# Making predictions with the test set filtered to top 10 features\n",
        "y_pred_filtered = clf_filtered.predict(X_test_filtered)\n",
        "# clf is a classifier.\n",
        "clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "print(\"Decision Tree Accuracy with Top 20 Features 20/80:\", accuracy_score_filtered)"
      ],
      "id": "030eb86b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "    #70, .30, 66, .34, 80,.20\n",
        "    #     x='parcel',\n",
        "#     y='abstrpod',\n",
        "#     color='yrbuilt',\n",
        "#     title='Does age affect whether a passenger survived?',\n",
        "#     # marginal=\"box\", # or violin, rug\n",
        "#     # hover_data=\n",
        "# )\n",
        "clf = RandomForestClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "id": "7404fc7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# clf is a classifier.\n",
        "clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "# Decision matrix 0 and 1. \n",
        "\n",
        "# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)"
      ],
      "id": "e8986801",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# clf is a classifier.\n",
        "clf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n",
        "#linear regression, \n",
        "clf = clf.fit(X_train, y_train)\n",
        "# Fit: fit the model to said data\n",
        "y_pred = clf.predict(X_test)\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "# %% this is the y_pred and y_probes from cell above\n",
        "print(metrics.classification_report(y_pred, y_test))\n",
        "accuracy_score(y_test, y_pred)\n",
        "# Decision matrix 0 and 1. \n",
        "\n",
        "# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)"
      ],
      "id": "48cbf5e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## QUESTION|TASK 3\n",
        "\n",
        "__Justify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.__\n",
        "\n",
        "_This Chart shows the top 20 features I selected. The reason Why these were selected were by the % influence they had on the learning model. Everything after status_I was under a % influence and would start to negativly impact the % accuracy of the models. _\n"
      ],
      "id": "3b3e1ec4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: Q3\n",
        "#| code-summary: Read and format data\n",
        "# Include and execute your code here\n",
        "\n",
        "#%%\n",
        "chart = px.bar(df_features.head(20), #You can change this 10 based on top features you want to see.\n",
        "    x='f_values', \n",
        "    y='f_names'\n",
        ")\n",
        "\n",
        "chart.update_layout(yaxis={'categoryorder':'total ascending'})\n",
        "# 0-1 is based on % 1 is 100% \n",
        "# f(x) = y (y is target, does not change here)\n",
        "# f is algorithm, Try 4\n",
        "# (x) is features "
      ],
      "id": "Q3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question|Task 4\n",
        "__Describe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.__"
      ],
      "id": "f090cac4"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}