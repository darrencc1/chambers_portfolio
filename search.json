[
  {
    "objectID": "story_telling.html",
    "href": "story_telling.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "story_telling.html#title-2-header",
    "href": "story_telling.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "projects.html#repo-for-all-my-projects",
    "href": "projects.html#repo-for-all-my-projects",
    "title": "DS250 Projects",
    "section": "",
    "text": "Project 1\nProject 2\nProject 3\nProject 4\nProject 5",
    "crumbs": [
      "DS250 Projects"
    ]
  },
  {
    "objectID": "Projects/project4.html",
    "href": "Projects/project4.html",
    "title": "Client Report - [Prediction of Houses Built Before 1980 in Colorado]",
    "section": "",
    "text": "Show the code\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree \nfrom sklearn.metrics import RocCurveDisplay\n\nfrom sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom scipy.stats import uniform, randint\n\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#elevator-pitch",
    "href": "Projects/project4.html#elevator-pitch",
    "title": "Client Report - [Prediction of Houses Built Before 1980 in Colorado]",
    "section": "Elevator pitch",
    "text": "Elevator pitch\npaste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n::: {#cell-project data .cell execution_count=3}\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\ndwellings_ml = pd.read_csv('https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv')\n\ndf2 = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4dwellings/master/data-raw/dwellings_neighborhoods_ml/dwellings_neighborhoods_ml.csv\")\ndwellings_ml.head()\n\n\n\n\n\n\n\n\n\nparcel\nabstrprd\nlivearea\nfinbsmnt\nbasement\nyrbuilt\ntotunits\nstories\nnocars\nnumbdrm\n...\narcstyle_THREE-STORY\narcstyle_TRI-LEVEL\narcstyle_TRI-LEVEL WITH BASEMENT\narcstyle_TWO AND HALF-STORY\narcstyle_TWO-STORY\nqualified_Q\nqualified_U\nstatus_I\nstatus_V\nbefore1980\n\n\n\n\n0\n00102-08-065-065\n1130\n1346\n0\n0\n2004\n1\n2\n2\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n1\n00102-08-073-073\n1130\n1249\n0\n0\n2005\n1\n1\n1\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n2\n00102-08-078-078\n1130\n1346\n0\n0\n2005\n1\n2\n1\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n3\n00102-08-081-081\n1130\n1146\n0\n0\n2005\n1\n1\n0\n2\n...\n0\n0\n0\n0\n0\n1\n0\n1\n0\n0\n\n\n4\n00102-08-086-086\n1130\n1249\n0\n0\n2005\n1\n1\n1\n2\n...\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n\n\n\n\n5 rows × 51 columns\n\n\n:::\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-1",
    "href": "Projects/project4.html#questiontask-1",
    "title": "Client Report - [Prediction of Houses Built Before 1980 in Colorado]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCreate 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.\nThere are many factors that we can use to help our algorithms learn and predict homes that were made before 1980.\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .34, random_state = 66)\n\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\n    #70, .30, 66, .34, 80,.20\n    #     x='parcel',\n#     y='abstrpod',\n#     color='yrbuilt',\n#     title='Does age affect whether a passenger survived?',\n#     # marginal=\"box\", # or violin, rug\n#     # hover_data=\n# )\nclf = RandomForestClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\ny_pred = clf.predict(X_test)\n\n\nLooking at the chart we see that almost 25% was from one-story buildings. After this the effectivness of the potential relationships drop drastically. So figuring out which data is almost irrelevant, in this case meaning well below 1% helpts my algorithms learn and predict houses made before 1980.\n\n\nShow the code\n# clf is a classifier.\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n\n# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)\n\n\n\n\nShow the code\ndf_features = pd.DataFrame(\n    {'f_names': X_train.columns, \n    'f_values': clf.feature_importances_}).sort_values('f_values', ascending = False)\n\n\n\n\nShow the code\n#%%\nchart = px.bar(df_features.head(20), #You can change this 10 based on top features you want to see.\n    x='f_values', \n    y='f_names'\n)\n\nchart.update_layout(yaxis={'categoryorder':'total ascending'})\n# 0-1 is based on % 1 is 100% \n# f(x) = y (y is target, does not change here)\n# f is algorithm, Try 4\n# (x) is features",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-2",
    "href": "Projects/project4.html#questiontask-2",
    "title": "Client Report - [Prediction of Houses Built Before 1980 in Colorado]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nBuild a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\nBelow I will show you some different results based on test size as well as RandomForest Algorithm vs Decision Tree algorithm. This first test size is 34 and random state 66.\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .34, random_state = 66)\n\n    # Extract the top 10 feature names\n\n\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .34, random_state = 66)\n\n\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\n# clf_filtered = RandomForestClassifier()\n# clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())\n\n\n\n\nShow the code\n# Assuming df_features is already created and sorted by feature importance\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = RandomForestClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = RandomForestClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"RandomForest Accuracy with Top 20 Features 34/66:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.90      0.89      2847\n           1       0.94      0.93      0.94      4944\n\n    accuracy                           0.92      7791\n   macro avg       0.91      0.92      0.92      7791\nweighted avg       0.92      0.92      0.92      7791\n\nRandomForest Accuracy with Top 20 Features 34/66: 0.9246566551148762\n\n\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .34, random_state = 66)\n\n\n\n\nShow the code\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = tree.DecisionTreeClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n# clf is a classifier.\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\nprint(\"Decision Tree Accuracy with Top 20 Features 34/66:\", accuracy_score_filtered)\n# Decision matrix 0 and 1. \n\n# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)\n\n\n              precision    recall  f1-score   support\n\n           0       0.87      0.87      0.87      2894\n           1       0.92      0.92      0.92      4897\n\n    accuracy                           0.90      7791\n   macro avg       0.90      0.90      0.90      7791\nweighted avg       0.90      0.90      0.90      7791\n\nDecision Tree Accuracy with Top 20 Features 34/66: 0.9246566551148762\n\n\nThis next set of data shows how changing the test from 34|66 to 30|70 affected the accuracy. In both cases the accuracy dropped by .2 - .4% depending on the learning.\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .30, random_state = 70)\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\n# Assuming df_features is already created and sorted by feature importance\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = RandomForestClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = RandomForestClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"RandomForest Accuracy with Top 20 Features 30/70:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.90      0.90      2575\n           1       0.94      0.93      0.94      4299\n\n    accuracy                           0.92      6874\n   macro avg       0.92      0.92      0.92      6874\nweighted avg       0.92      0.92      0.92      6874\n\nRandomForest Accuracy with Top 20 Features 30/70: 0.925516438754728\n\n\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .30, random_state = 70)\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = tree.DecisionTreeClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n# clf is a classifier.\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\nprint(\"Decision Tree Accuracy with Top 20 Features 30/70:\", accuracy_score_filtered)\n# Decision matrix 0 and 1. \n\n# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)\n\n\n              precision    recall  f1-score   support\n\n           0       0.87      0.86      0.87      2627\n           1       0.91      0.92      0.92      4247\n\n    accuracy                           0.90      6874\n   macro avg       0.89      0.89      0.89      6874\nweighted avg       0.90      0.90      0.90      6874\n\nDecision Tree Accuracy with Top 20 Features 30/70: 0.925516438754728\n\n\nFor this set I am using the 80/20 test and with this the accuracy went back up reaching 92.45% this dropped to 92.34% When using only the top 10 and up to 92.77 if I used the top 20 for the random Tree algorithm which is the highest of all the tests so far.\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .2, random_state = 80)\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = tree.DecisionTreeClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n# clf is a classifier.\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\nprint(\"Decision Tree Accuracy with Top 20 Features 20/80:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.86      0.87      1746\n           1       0.91      0.93      0.92      2837\n\n    accuracy                           0.90      4583\n   macro avg       0.90      0.89      0.89      4583\nweighted avg       0.90      0.90      0.90      4583\n\nDecision Tree Accuracy with Top 20 Features 20/80: 0.925516438754728\n\n\n\n\nShow the code\n    #70, .30, 66, .34, 80,.20\n    #     x='parcel',\n#     y='abstrpod',\n#     color='yrbuilt',\n#     title='Does age affect whether a passenger survived?',\n#     # marginal=\"box\", # or violin, rug\n#     # hover_data=\n# )\nclf = RandomForestClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\ny_pred = clf.predict(X_test)\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n\n\n              precision    recall  f1-score   support\n\n           0       0.90      0.89      0.90      1725\n           1       0.93      0.94      0.94      2858\n\n    accuracy                           0.92      4583\n   macro avg       0.92      0.92      0.92      4583\nweighted avg       0.92      0.92      0.92      4583\n\n\n\n0.921885228016583\n\n\n\n\nShow the code\n# clf is a classifier.\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Decision matrix 0 and 1. \n\n# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)\n\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.86      0.87      1742\n           1       0.92      0.93      0.92      2841\n\n    accuracy                           0.90      4583\n   macro avg       0.90      0.90      0.90      4583\nweighted avg       0.90      0.90      0.90      4583\n\n\n\n0.9037748199869081\n\n\n\n\nShow the code\n# clf is a classifier.\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Decision matrix 0 and 1. \n\n# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)\n\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.86      0.87      1741\n           1       0.91      0.93      0.92      2842\n\n    accuracy                           0.90      4583\n   macro avg       0.90      0.89      0.89      4583\nweighted avg       0.90      0.90      0.90      4583\n\n\n\n0.9000654593061314\n\n\n\n\nShow the code\ndf.join(df2, lsuffix=\"parcel\", rsuffix=\"parcel\")\n# print(\"This is the joined dataframe of Neighborhood dwellings and dwellings ml. \")\n\n\n\n\n\n\n\n\n\nname\nyear\nAK\nAL\nAR\nAZ\nCA\nCO\nCT\nDC\n...\nnbhd_802\nnbhd_803\nnbhd_804\nnbhd_805\nnbhd_901\nnbhd_902\nnbhd_903\nnbhd_904\nnbhd_905\nnbhd_906\n\n\n\n\n0\nAaden\n2005\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\nAaden\n2007\n0.0\n5.0\n0.0\n5.0\n20.0\n6.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\nAaden\n2008\n0.0\n15.0\n13.0\n20.0\n135.0\n10.0\n9.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\nAaden\n2009\n0.0\n28.0\n20.0\n23.0\n158.0\n22.0\n12.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\nAaden\n2010\n0.0\n8.0\n6.0\n12.0\n62.0\n9.0\n5.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n393379\nZyon\n2011\n0.0\n0.0\n0.0\n6.0\n0.0\n0.0\n0.0\n0.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n393380\nZyon\n2012\n0.0\n0.0\n0.0\n0.0\n5.0\n0.0\n0.0\n0.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n393381\nZyon\n2013\n0.0\n0.0\n0.0\n0.0\n6.0\n0.0\n0.0\n0.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n393382\nZyon\n2014\n0.0\n7.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n393383\nZyon\n2015\n0.0\n0.0\n0.0\n0.0\n7.0\n0.0\n0.0\n0.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n393384 rows × 328 columns\n\n\n\n\n\nShow the code\nprint(\"This is the joined dataframe of Neighborhood dwellings and dwellings ml. \")\n\n\nThis is the joined dataframe of Neighborhood dwellings and dwellings ml. \n\n\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .34, random_state = 66)\n\n    # Extract the top 10 feature names\n\n\n# X_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\n# y_pred = dwellings_ml.filter(regex = \"before1980\")\n# X_train, X_test, y_train, y_test = train_test_split(\n#     X_pred, y_pred, test_size = .34, random_state = 66)\n\n\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\n# clf_filtered = RandomForestClassifier()\n# clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel()) \n\n\n\n\nShow the code\n# Assuming df_features is already created and sorted by feature importance\ntop_20_features = df_features.head(10)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = RandomForestClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = RandomForestClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"RandomForest Accuracy with Top 20 Features, 34/66 and joined with neighborhood dwellings  (df2):\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.91      0.90      2841\n           1       0.95      0.93      0.94      4950\n\n    accuracy                           0.92      7791\n   macro avg       0.92      0.92      0.92      7791\nweighted avg       0.92      0.92      0.92      7791\n\nRandomForest Accuracy with Top 20 Features, 34/66 and joined with neighborhood dwellings  (df2): 0.8926967013220383\n\n\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .34, random_state = 66)\n\n\n\n\nShow the code\nclf_filtered = tree.DecisionTreeClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel()) \n# print(metrics.classification_report(y_pred, y_test))\n# accuracy_score(y_test, y_pred)\n\n\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .34, random_state = 66)\n\n\n\n\nShow the code\ntop_20_features = df_features.head(10)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\n# clf_filtered = tree.DecisionTreeClassifier()\n# clf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n# clf is a classifier.\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\nprint(\"Decision Tree Accuracy with Top 20 Features 34/66:\", accuracy_score_filtered)\n# Decision matrix 0 and 1. \n\n# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)\n\n\n              precision    recall  f1-score   support\n\n           0       0.87      0.87      0.87      2922\n           1       0.92      0.92      0.92      4869\n\n    accuracy                           0.90      7791\n   macro avg       0.89      0.89      0.89      7791\nweighted avg       0.90      0.90      0.90      7791\n\nDecision Tree Accuracy with Top 20 Features 34/66: 0.8926967013220383\n\n\nThis next set of data shows how changing the test from 34|66 to 30|70 affected the accuracy. In both cases the accuracy dropped by .2 - .4% depending on the learning.\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .30, random_state = 70)\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\n# Assuming df_features is already created and sorted by feature importance\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = RandomForestClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n\nclf = RandomForestClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Evaluating the model with the accuracy score using Top 10 Features\naccuracy_score_filtered = accuracy_score(y_test, y_pred_filtered)\nprint(\"RandomForest Accuracy with Top 20 Features 30/70:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.89      0.90      0.90      2577\n           1       0.94      0.93      0.94      4297\n\n    accuracy                           0.92      6874\n   macro avg       0.92      0.92      0.92      6874\nweighted avg       0.92      0.92      0.92      6874\n\nRandomForest Accuracy with Top 20 Features 30/70: 0.9242071574047134\n\n\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .30, random_state = 70)\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = tree.DecisionTreeClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n# clf is a classifier.\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\nprint(\"Decision Tree Accuracy with Top 20 Features 30/70:\", accuracy_score_filtered)\n# Decision matrix 0 and 1. \n\n# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)\n\n\n              precision    recall  f1-score   support\n\n           0       0.87      0.86      0.86      2637\n           1       0.91      0.92      0.92      4237\n\n    accuracy                           0.90      6874\n   macro avg       0.89      0.89      0.89      6874\nweighted avg       0.90      0.90      0.90      6874\n\nDecision Tree Accuracy with Top 20 Features 30/70: 0.9242071574047134\n\n\nFor this set I am using the 80/20 test and with this the accuracy went back up reaching 92.45% this dropped to 92.34% When using only the top 10 and up to 92.77 if I used the top 20 for the random Tree algorithm which is the highest of all the tests so far.\n\n\nShow the code\nX_pred = dwellings_ml.drop(dwellings_ml.filter(regex = 'before1980|yrbuilt|parcel').columns, axis = 1)\ny_pred = dwellings_ml.filter(regex = \"before1980\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pred, y_pred, test_size = .2, random_state = 80)\n    #70, .30, 66, .34, 80,.20\n\n\n\n\nShow the code\ntop_20_features = df_features.head(20)['f_names'].tolist()\n\n# Now, filter X_train and X_test to include only the top 10 features\nX_train_filtered = X_train[top_20_features]\nX_test_filtered = X_test[top_20_features]\n\n# Training your model using these filtered datasets\nclf_filtered = tree.DecisionTreeClassifier()\nclf_filtered = clf_filtered.fit(X_train_filtered, y_train.values.ravel())  # .values.ravel() to convert y_train to 1D array if needed\n\n# Making predictions with the test set filtered to top 10 features\ny_pred_filtered = clf_filtered.predict(X_test_filtered)\n# clf is a classifier.\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\nprint(\"Decision Tree Accuracy with Top 20 Features 20/80:\", accuracy_score_filtered)\n\n\n              precision    recall  f1-score   support\n\n           0       0.87      0.86      0.86      1745\n           1       0.91      0.92      0.92      2838\n\n    accuracy                           0.90      4583\n   macro avg       0.89      0.89      0.89      4583\nweighted avg       0.90      0.90      0.90      4583\n\nDecision Tree Accuracy with Top 20 Features 20/80: 0.9242071574047134\n\n\n\n\nShow the code\n    #70, .30, 66, .34, 80,.20\n    #     x='parcel',\n#     y='abstrpod',\n#     color='yrbuilt',\n#     title='Does age affect whether a passenger survived?',\n#     # marginal=\"box\", # or violin, rug\n#     # hover_data=\n# )\nclf = RandomForestClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\ny_pred = clf.predict(X_test)\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n\n\n              precision    recall  f1-score   support\n\n           0       0.90      0.89      0.90      1720\n           1       0.94      0.94      0.94      2863\n\n    accuracy                           0.92      4583\n   macro avg       0.92      0.92      0.92      4583\nweighted avg       0.92      0.92      0.92      4583\n\n\n\n0.9216670303294785\n\n\n\n\nShow the code\n# clf is a classifier.\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Decision matrix 0 and 1. \n\n# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)\n\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.86      0.87      1752\n           1       0.91      0.93      0.92      2831\n\n    accuracy                           0.90      4583\n   macro avg       0.90      0.89      0.89      4583\nweighted avg       0.90      0.90      0.90      4583\n\n\n\n0.8994108662448178\n\n\n\n\nShow the code\n# clf is a classifier.\nclf = tree.DecisionTreeClassifier() #Rand Forest is similar, try it. \n#linear regression, \nclf = clf.fit(X_train, y_train)\n# Fit: fit the model to said data\ny_pred = clf.predict(X_test)\ny_probs = clf.predict_proba(X_test)\n# %% this is the y_pred and y_probes from cell above\nprint(metrics.classification_report(y_pred, y_test))\naccuracy_score(y_test, y_pred)\n# Decision matrix 0 and 1. \n\n# XGBoost no lower than 66/34, 70/30 or 80/20 (This is almost too much)\n\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.86      0.87      1752\n           1       0.91      0.93      0.92      2831\n\n    accuracy                           0.90      4583\n   macro avg       0.90      0.89      0.89      4583\nweighted avg       0.90      0.90      0.90      4583\n\n\n\n0.8998472616190268",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-3",
    "href": "Projects/project4.html#questiontask-3",
    "title": "Client Report - [Prediction of Houses Built Before 1980 in Colorado]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nJustify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.\nThis Chart shows the top 20 features I selected. The reason Why these were selected were by the % influence they had on the learning model. Everything after status_I was under a % influence and would start to negativly impact the % accuracy of the models. \n\n\nRead and format data\n# Include and execute your code here\n\n#%%\nchart = px.bar(df_features.head(20), #You can change this 10 based on top features you want to see.\n    x='f_values', \n    y='f_names'\n)\n\nchart.update_layout(yaxis={'categoryorder':'total ascending'})\n# 0-1 is based on % 1 is 100% \n# f(x) = y (y is target, does not change here)\n# f is algorithm, Try 4\n# (x) is features",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project4.html#questiontask-4",
    "href": "Projects/project4.html#questiontask-4",
    "title": "Client Report - [Prediction of Houses Built Before 1980 in Colorado]",
    "section": "Question|Task 4",
    "text": "Question|Task 4\nDescribe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.",
    "crumbs": [
      "DS250 Projects",
      "Project 4"
    ]
  },
  {
    "objectID": "Projects/project2.html",
    "href": "Projects/project2.html",
    "title": "Client Report - [Airline Delays by Airport for the U.S]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\n# df = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\")\n# flight_delays = df\n# flight_delays\n# print(flight_delays.columns)\n\n# replace 999 and -999 with the mean, 1500+ replace w/ 1500. \n# \n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#elevator-pitch",
    "href": "Projects/project2.html#elevator-pitch",
    "title": "Client Report - [Airline Delays by Airport for the U.S]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\n# df = pd.read_csv(\"https://raw.githubusercontent.com/byuidatascience/data4missing/master/data-raw/flights_missing/flights_missing.json\")\n# flight_delays = df\n# flight_delays\n# print(flight_delays.columns)\n\n# replace 999 and -999 with the mean, 1500+ replace w/ 1500. \n# \n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-1",
    "href": "Projects/project2.html#questiontask-1",
    "title": "Client Report - [Airline Delays by Airport for the U.S]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nFix all of the varied missing data types in the data to be consistent (all missing values should be displayed as “NaN”)\ntype your results and analysis here\n\n\nShow the code\n# flight_delays = pd.json_normalize(flight_delays)\n\n# # Now, you should have separate columns for each key in the JSON objects\n# # You can check the column names again to verify2\n# print(flight_delays.columns)\n\n\n\n\nShow the code\n# %% assign function lets each of these columns get added to the data frame. \n# makes the new column = to another column in the df. (Like excel)\n# So minutes total delays will be another affed colimn at the end (severe)\n# lambda just returns whatever you do to x, it is an inline function. \n# weather = (flight_delays.assign(\n#     severe = flight_delays.num_of_delays_weather # no# missing, severe \n#     nodla_nona = lambda x: (x.num_of_delays_late_aircraft\n#         .replace(-999, np.nan)), #missing is -999, this is taking any missing data and replacing it with nan, copies the data but gives nan for 999\n#     mild_late = lambda x: x.nodla_nona.fillna(x.nodla_nona.mean())*0.3,\n#     mild = np.where(\n#         flight_delays.month.isin(['April', 'May', 'June', 'July', 'August']), \n#             flight_delays.num_of_delays_nas*0.4, \n#             flight_delays.num_of_delays_nas*0.65),\n#     weather = lambda x: x.severe + x.mild_late + x.mild,\n#     proportion_weather_delay = lambda x: x.weather / x.num_of_delays_total,\n#     proportion_weather_total = lambda x:  x.weather / x.num_of_flights_total)\n#     .filter(['airport_code','month','year', 'severe','mild', 'mild_late',\n#     'weather', 'proportion_weather_total', \n#     'proportion_weather_delay', 'num_of_flights_total', 'num_of_delays_total']))\n# weather.head()\n# weather = (flight_delays.assign(\n#     severe = flight_delays.num_of_delays_weather, # no# missing, severe \n#     nodla_nona = lambda x: (x.num_of_delays_late_aircraft\n#         .replace(-999, np.nan)), #missing is -999, this is taking any missing data and replacing it with nan, copies the data but gives nan for 999\n#     mild_late = lambda x: x.nodla_nona.fillna(x.nodla_nona.mean())*0.3,\n#     mild = np.where(\n#         flight_delays.month.isin(['April', 'May', 'June', 'July', 'August']), \n#             flight_delays.num_of_delays_nas*0.4, \n#             flight_delays.num_of_delays_nas*0.65),\n#     weather = lambda x: x.severe + x.mild_late + x.mild,\n#     proportion_weather_delay = lambda x: x.weather / x.num_of_delays_total,\n#     proportion_weather_total = lambda x:  x.weather / x.num_of_flights_total)\n#     .filter(['airport_code','month','year', 'severe','mild', 'mild_late',\n#     'weather', 'proportion_weather_total', \n#     'proportion_weather_delay', 'num_of_flights_total', 'num_of_delays_total']))\n# weather.head()\n\n\n\n_include figures in chunks and discuss your findings in the figure._\n\n\n\n\n\n## QUESTION|TASK 2\n\n__Which airport has the worst delays?__\n\n_type your results and analysis here_\n\n::: {#q2 .cell execution_count=5}\n``` {.python .cell-code code-summary=\"Read and format data\"}\n# Include and execute your code here\n:::\n\n_include figures in chunks and discuss your findings in the figure._\n\n\n\n\n\n\n## QUESTION|TASK 3\n\n__What is the best month to fly if you want to avoid delays of any length?__\n\n\n\n\ninclude figures in chunks and discuss your findings in the figure.\n#| label: Q3 table # #| code-summary: table example # #| tbl-cap: “Not much of a table” # #| tbl-cap-location: top # # Include and execute your code here # mydat = df.head(1000)\n# .groupby(‘year’)\n# .sum()\n# .reset_index()\n# .tail(10)\n# .filter([“year”, “AK”,“AR”])",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-4",
    "href": "Projects/project2.html#questiontask-4",
    "title": "Client Report - [Airline Delays by Airport for the U.S]",
    "section": "QUESTION|TASK 4",
    "text": "QUESTION|TASK 4\nYour job is to create a new column that calculates the total number of flights delayed by weather (both severe and mild).",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "Projects/project2.html#questiontask-5",
    "href": "Projects/project2.html#questiontask-5",
    "title": "Client Report - [Airline Delays by Airport for the U.S]",
    "section": "QUESTION|TASK 5",
    "text": "QUESTION|TASK 5\nUsing the new weather variable calculated above, create a barplot showing the proportion of all flights that are delayed by weather at each airport. Discuss what you learn from this graph.",
    "crumbs": [
      "DS250 Projects",
      "Project 2"
    ]
  },
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "ml.html#title-2-header",
    "href": "ml.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html",
    "href": "full_stack.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "full_stack.html#title-2-header",
    "href": "full_stack.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html",
    "href": "competition.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "competition.html#title-2-header",
    "href": "competition.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html",
    "href": "cleansing.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "cleansing.html#title-2-header",
    "href": "cleansing.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "exploration.html#title-2-header",
    "href": "exploration.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "index.html#title-2-header",
    "href": "index.html#title-2-header",
    "title": "about me",
    "section": "",
    "text": "MarkDown Basics"
  },
  {
    "objectID": "Projects/project1.html",
    "href": "Projects/project1.html",
    "title": "Client Report - [Usage of Names over time]",
    "section": "",
    "text": "With this data we can see the change in popularity of names over time. An example of this is with the names Mary, Martha, Paul and Peter, they lost popularity by an average of 31.25% per year from 1957 to the year 2000. \n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#elevator-pitch",
    "href": "Projects/project1.html#elevator-pitch",
    "title": "Client Report - [Usage of Names over time]",
    "section": "",
    "text": "With this data we can see the change in popularity of names over time. An example of this is with the names Mary, Martha, Paul and Peter, they lost popularity by an average of 31.25% per year from 1957 to the year 2000. \n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-1",
    "href": "Projects/project1.html#questiontask-1",
    "title": "Client Report - [Usage of Names over time]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nHow does your name at your birth year compare to its use historically?\nIn the year 1947 there were only 5 children named Darren. The name started to gain popularity in 1958 with 385 kids given the name and peaking at 5931 children given this name in 1965. I was born in the year 1996 where the popularity of the name Darren had declined by 81% from 1947. \n\n\nRead and format data\n# Include and execute your code here\n #and \"va != 0\"])\n# df.query(\"year &lt;= 2000\") \n\n# year with lowest babies names Darren\n# pd.unique(df.query(\"name == 'Darren' \").year).min()\n# pd.unique(df.query(\"name == 'Darren' \").year).max()\n\n# Gives the size, so rows times columns so with year, how many years you have, name: how many times the specified name occurs. \n# pd.unique(df.query(\"name == 'Darren' \").year).size\ndf.query(\"name == 'Darren'\").filter([\"name\", \"year\", \"Total\"]).query(\"year &gt; 1990 and year &lt; 2000\")\n\n\n\n\n\n\n\n\n\nname\nyear\nTotal\n\n\n\n\n90941\nDarren\n1991\n1277.0\n\n\n90942\nDarren\n1992\n1300.0\n\n\n90943\nDarren\n1993\n1419.0\n\n\n90944\nDarren\n1994\n1431.0\n\n\n90945\nDarren\n1995\n1271.0\n\n\n90946\nDarren\n1996\n1109.0\n\n\n90947\nDarren\n1997\n989.0\n\n\n90948\nDarren\n1998\n997.0\n\n\n90949\nDarren\n1999\n1001.0\n\n\n\n\n\n\n\nThe green line indicates when I was born. \n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot example\n# Include and execute your code here\ndarren_df = df[df['name'] == 'Darren']\nchart = px.bar(darren_df.head(200),\n    x=\"year\", \n    y=\"Total\",\n    title = \"Usage of the name Darren\",\n    labels = {\"year\":\"Year\"}\n\n)\nhighlight_year = 1996\n\n# Add a vertical line\nchart.add_shape(\n    type=\"line\",\n    x0=highlight_year,\n    y0=darren_df['Total'].min(),\n    x1=highlight_year,\n    y1=darren_df['Total'].max(),\n    line=dict(\n        color=\"green\",\n        width=3,\n        dash=\"dashdot\"\n    )\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-2",
    "href": "Projects/project1.html#questiontask-2",
    "title": "Client Report - [Usage of Names over time]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nIf you talked to someone named Brittany on the phone, what is your guess of his or her age? What ages would you not guess?\nI would guess they would be born between the years 1985 and 1996. Which would be between the ages of 27 and 38. The reason why I would choose this range of age is because the majority of people named brittany were born in these years. 241847 out of 327718 were born in these years. That is 73.8% of people named Brittany in a 11 year period.I would not choose any date from 1968 - 1984 and 1997 and 2015 because that is only 26.2 % of people name Brittany were born in these years. \n\n\nRead and format data\n# Include and execute your code here\n\ndf.query(\"name == 'Brittany'\").filter([\"name\", \"year\",\n \"Total\"]).query(\"year &gt; 1984 and year &lt; 1997\")\n\n\n\n\n\n\n\n\n\nname\nyear\nTotal\n\n\n\n\n53222\nBrittany\n1985\n14010.0\n\n\n53223\nBrittany\n1986\n17856.5\n\n\n53224\nBrittany\n1987\n18825.5\n\n\n53225\nBrittany\n1988\n21952.0\n\n\n53226\nBrittany\n1989\n30848.0\n\n\n53227\nBrittany\n1990\n32562.5\n\n\n53228\nBrittany\n1991\n26963.5\n\n\n53229\nBrittany\n1992\n23416.5\n\n\n53230\nBrittany\n1993\n21728.0\n\n\n53231\nBrittany\n1994\n17808.5\n\n\n53232\nBrittany\n1995\n15875.5\n\n\n53233\nBrittany\n1996\n13796.0\n\n\n\n\n\n\n\nThis is isolating the years that hold 73.8% of people named Brittany\n::: {#cell-Q2 chart .cell execution_count=6}\n\nplot example\n# Include and execute your code here\nfiltered_df = df[df['name'] == 'Brittany']\nchart = px.bar(filtered_df.head(200),\n    x=\"year\", \n    y=\"Total\",\n    title = \"Brittany name usage over the years\"\n)\nchart.add_shape(\n    type=\"line\",\n    x0= 1985,\n    y0=14000,\n    x1=1996,\n    y1=14000,\n    line=dict(\n        color=\"red\",\n        width=3,\n        dash=\"dashdot\")\n        )\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=7}\n\ntable example\n# Include and execute your code here\nmydat = filtered_df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n38\n2006\n0.0\n13.0\n\n\n39\n2007\n0.0\n12.0\n\n\n40\n2008\n0.0\n0.0\n\n\n41\n2009\n0.0\n6.0\n\n\n42\n2010\n0.0\n0.0\n\n\n43\n2011\n0.0\n7.0\n\n\n44\n2012\n0.0\n13.0\n\n\n45\n2013\n0.0\n0.0\n\n\n46\n2014\n0.0\n5.0\n\n\n47\n2015\n0.0\n6.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-3",
    "href": "Projects/project1.html#questiontask-3",
    "title": "Client Report - [Usage of Names over time]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nMary, Martha, Peter, and Paul are all Christian names. From 1920 - 2000, compare the name usage of each of the four names. What trends do you notice?\n_These four names follow the same trend line in popularity over time. This may be due to cultural shifts in the popularity of christianity. Between 1947 and 1957 was the peak of usage of these names with an average usage of 201149 This drops relativly quickly over the years with an average percent decrese of 31.45% over 43 years.\n::: {#cell-Q3 chart .cell execution_count=8}\n\nplot example\n# Include and execute your code here\ndesired_names = [\"Mary\", \"Martha\", \"Peter\", \"Paul\"]\n#List of names I want to see in chart\n#isin is boolean check. Is the four names in the column?\n# filter_df = df.name.isin(desired_names)\n# filter_df = df[df[\"name\"].isin(desired_names)]\nfilter_df = df[df.name.isin(desired_names)].query(\"year &gt; 1919 and year &lt; 2001\")\n#takes filter and query filter_df variable and applies to the chart. \nchart = px.bar(filter_df,\n    x=\"year\", \n    y=\"Total\",\n    color = \"name\",\n    title = \"Comparison of Martha, Mary, Peter and Paul over the years\",\n    labels = {\"year\": \"Year\"}\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=9}\n\ntable example\n# Include and execute your code here\n\n# Taking instances of each record and adding them together.\nmydat = (df\n    [df.name.isin(desired_names)]\n    .query('year &gt; 1947 and year &lt; 1957')\n    .groupby('name')\n    .agg(total = ('Total', 'sum'))\n    .reset_index()\n)\n\nmydat.head(50)\n\n\n\n# mydat = filter_df.head(1000)\\\n#     .groupby('year')\\\n#     .sum()\\\n#     .reset_index()\\\n#     .tail(10)\\\n#     .filter([\"year\", \"AK\",\"AR\"])\n\n# display(mydat)\n\n\n\n\n\n\n\n\n\nname\ntotal\n\n\n\n\n0\nMartha\n82131.0\n\n\n1\nMary\n437024.5\n\n\n2\nPaul\n203619.0\n\n\n3\nPeter\n81825.5\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project1.html#questiontask-4",
    "href": "Projects/project1.html#questiontask-4",
    "title": "Client Report - [Usage of Names over time]",
    "section": "Question|Task 4",
    "text": "Question|Task 4\nThink of a unique name from a famous movie. Plot the usage of that name and see how changes line up with the movie release. Does it look like the movie had an effect on usage?\n_I chose the name Allison from Raiders of the Lost Ark. This movie released in 1981, The name began to gain popularity and over 4 years increase by 43.36%. The movie was the most popular movie of 1981 and so would potentially affect the usage of names over the next couple of years.\n\n\nShow the code\nmovie_df = df[df['name'] == 'Allison']\nchart = px.bar(movie_df.head(200),\n    x=\"year\", \n    y=\"Total\",\n    title = \"Allison name usage over the years\",\n    labels = {\"year\": \"Year\"}\n)\nchart.add_shape(\n    type=\"line\",\n    x0= 1981,\n    y0=4990,\n    x1=1984,\n    y1=7000,\n    line=dict(\n        color=\"red\",\n        width=3,\n        dash=\"dashdot\")\n        )\nchart.show()",
    "crumbs": [
      "DS250 Projects",
      "Project 1"
    ]
  },
  {
    "objectID": "Projects/project3.html",
    "href": "Projects/project3.html",
    "title": "Client Report - [Basebal Over the Years]",
    "section": "",
    "text": "Show the code\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\nq = 'SELECT * FROM allstarfull LIMIT 5'\nresults = pd.read_sql_query(q,con)\n\n# results",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#elevator-pitch",
    "href": "Projects/project3.html#elevator-pitch",
    "title": "Client Report - [Basebal Over the Years]",
    "section": "Elevator pitch",
    "text": "Elevator pitch\npaste your elevator pitch here When considering baseball players it is important to look at their average hits. Within this average we see that the number of total hits and At bats is very important for choosing players for a team. Somebody with a couple thousand at bats but a lower % is better than someone with 10 at bats and 60% hit rate. \nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-1",
    "href": "Projects/project3.html#questiontask-1",
    "title": "Client Report - [Basebal Over the Years]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nWrite an SQL query to create a new dataframe about baseball players who attended BYU-Idaho. The new table should contain five columns: playerID, schoolID, salary, and the yearID/teamID associated with each salary. Order the table by salary (highest to lowest) and print out the table in your report.\nI made sure to only display unique values in this table, meaning I used distinct to ensure there were no repeating values and it is ordered by salary. We can see that there are only 2 players who had attented byui.\n\n\nShow the code\nq = '''SELECT DISTINCT cp.playerID, cp.schoolID, s.salary, s.yearID, s.teamID\nFROM collegeplaying cp\nJOIN salaries s ON cp.playerID = s.playerID\nWHERE cp.schoolID = 'idbyuid'\nORDER BY s.salary DESC'''\n\nresult = pd.read_sql_query(q, con)\n\nprint(result)\n\n\n     playerID schoolID     salary  yearID teamID\n0   lindsma01  idbyuid  4000000.0    2014    CHA\n1   lindsma01  idbyuid  3600000.0    2012    BAL\n2   lindsma01  idbyuid  2800000.0    2011    COL\n3   lindsma01  idbyuid  2300000.0    2013    CHA\n4   lindsma01  idbyuid  1625000.0    2010    HOU\n5   stephga01  idbyuid  1025000.0    2001    SLN\n6   stephga01  idbyuid   900000.0    2002    SLN\n7   stephga01  idbyuid   800000.0    2003    SLN\n8   stephga01  idbyuid   550000.0    2000    SLN\n9   lindsma01  idbyuid   410000.0    2009    FLO\n10  lindsma01  idbyuid   395000.0    2008    FLO\n11  lindsma01  idbyuid   380000.0    2007    FLO\n12  stephga01  idbyuid   215000.0    1999    SLN\n13  stephga01  idbyuid   185000.0    1998    PHI\n14  stephga01  idbyuid   150000.0    1997    PHI\n\n\n\n\nShow the code\nsqlite_file = 'lahmansbaseballdb.sqlite'\ncon = sqlite3.connect(sqlite_file)\n\nq = 'SELECT * FROM allstarfull LIMIT 5'\nresults = pd.read_sql_query(q,con)\n\nresults\n\n\n\n\n\n\n\n\n\nID\nplayerID\nyearID\ngameNum\ngameID\nteamID\nteam_ID\nlgID\nGP\nstartingPos\n\n\n\n\n0\n1\ngomezle01\n1933\n0\nALS193307060\nNYA\n921\nAL\n1\n1\n\n\n1\n2\nferreri01\n1933\n0\nALS193307060\nBOS\n912\nAL\n1\n2\n\n\n2\n3\ngehrilo01\n1933\n0\nALS193307060\nNYA\n921\nAL\n1\n3\n\n\n3\n4\ngehrich01\n1933\n0\nALS193307060\nDET\n919\nAL\n1\n4\n\n\n4\n5\ndykesji01\n1933\n0\nALS193307060\nCHA\n915\nAL\n1\n5\n\n\n\n\n\n\n\n\n\nShow the code\n# Use pandas to read the SQL query into a DataFrame\n# df = pd.read_sql_query(q, con)\n\n# # Close the SQLite database connection\n# #con.close()\n\n# # Print the DataFrame\n# print(df)",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-2",
    "href": "Projects/project3.html#questiontask-2",
    "title": "Client Report - [Basebal Over the Years]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nThis three-part question requires you to calculate batting average (number of hits divided by the number of at-bats)\nWrite an SQL query that provides playerID, yearID, and batting average for players with at least 1 at bat that year. Sort the table from highest batting average to lowest, and then by playerid alphabetically. Show the top 5 results in your report.\nUse the same query as above, but only include players with at least 10 at bats that year. Print the top 5 results.\nNow calculate the batting average for players over their entire careers (all years combined). Only include players with at least 100 at bats, and print the top 5 results.\n_The top hitters in this situation have an average hit rate of 100%. If you look in the H (hits) row you will see they have only hit once, as well as been at bat once.\n\n\nShow the code\nq = '''\nSELECT playerID, yearID, H, AB, (H * 100) / AB * 1.0 AS \"Average Hits %\"\nFROM batting\nWHERE AB &gt;= 1 \nGROUP BY playerID\nORDER BY H * 100/AB *1.0 DESC, playerID \nLIMIT 5'''\n\nresult = pd.read_sql_query(q, con)\n\nprint(result)\n\n\n    playerID  yearID  H  AB  Average Hits %\n0  abramge01    1923  1   1           100.0\n1  alanirj01    2019  1   1           100.0\n2  alberan01    2017  1   1           100.0\n3  allarko01    2018  1   1           100.0\n4  banisje01    1991  1   1           100.0\n\n\nWhen you take the At Bats and increase it to at least 10 times you see that the highest stat is 64% instead of 100% like the previous chart. These percentiles are much more useful as they have at least some data behind them. These top five batters all were at bat 11-16 times.\n\n\nShow the code\nq = '''\nSELECT playerID, yearID, H, AB, (H * 100) / AB * 1.0 AS \"Average Hits %\"\nFROM batting\nWHERE AB &gt;= 10 \nGROUP BY playerID\nORDER BY H * 100/AB *1.0 DESC \nLIMIT 5'''\n\nresult = pd.read_sql_query(q, con)\n\nprint(result)\n\n\n    playerID  yearID  H  AB  Average Hits %\n0  nymanny01    1974  9  14            64.0\n1  silvech01    1948  8  14            57.0\n2  puccige01    1930  9  16            56.0\n3  applepe01    1927  6  11            54.0\n4  ariasjo01    2006  6  11            54.0\n\n\n_In this last set of data you see the Average Hits go down even further from before. However, this is the most useful of the charts. All of these batters had well over 100 bats, meaning that they have many actual data points to use for their average hits. If you were to look at players you were interested in hiring for a team, the amount of Hits and at bats influencing the Average hits would be important to note. Concistency over long periods / many data points and average hits at 49% would be much more indicative of good hitters than someone who has been at bat 14 times and has a 64% average hit rate.\n\n\nRead and format data\n# Include and execute your code here\n\nq = \"\"\" \nSELECT playerID, sum(H), sum(AB), (SUM(H) * 100) / sum(AB) * 1.0 AS \"Average hits %\"\nFROM batting \nGROUP BY playerID \nHAVING SUM(AB) &gt;= 100\nORDER BY  sum(H) * 100/ sum(AB) *1.0 DESC\nLIMIT 5\n\"\"\"\nresult = pd.read_sql_query(q, con)\n\nprint(result) \n\n\n    playerID  sum(H)  sum(AB)  Average hits %\n0   cobbty01    4189    11436            36.0\n1  barnero01     860     2391            35.0\n2  hornsro01    2930     8173            35.0\n3  jacksjo01    1772     4981            35.0\n4   kingst01      96      272            35.0",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project3.html#questiontask-3",
    "href": "Projects/project3.html#questiontask-3",
    "title": "Client Report - [Basebal Over the Years]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nPick any two baseball teams and compare them using a metric of your choice (average salary, home runs, number of wins, etc). Write an SQL query to get the data you need, then make a graph using Plotly Express to visualize the comparison. What do you learn?\nPhilidelfia Phillies vs New York Yankies for 10 years. That is 2006 - 2016. I chose these two popular teams to see how they average over time. This chart shows their average Home Runs with the New York Yankies averaging higher by 2%.\n\n\nShow the code\nq= '''SELECT DISTINCT teamID, \n       SUM(HR) AS Total_Home_Runs, \n       SUM(H) AS Total_Hits, \n       SUM(AB) AS Total_At_Bats, \n       (SUM(HR) * 100.0) / SUM(H) AS \"Average Home Runs %\"\nFROM teams\nWHERE teamID IN ('PHI', 'NYA') AND yearID &gt;= 2006\nGROUP BY teamID\nORDER BY \"Average_Home_Runs_Percentage\" DESC'''\n\nresult = pd.read_sql_query(q, con)\n\nprint(result)\n\n\n  teamID  Total_Home_Runs  Total_Hits  Total_At_Bats  Average Home Runs %\n0    PHI             2475       19607          77718            12.623043\n1    NYA             3003       20554          77872            14.610295\n\n\nThis chart is looking at the total pay to players over ten years and the average pay of players over the same period of time. New York Yankes paid 2.26 billion dollars over 10 years to their players, this is 870million dollars more than the Phillidelphia Phillies\n\n\nShow the code\nq = '''SELECT teamID, \n              ROUND(SUM(salary), 3) AS Total_Player_Pay, \n              ROUND((SUM(salary) / COUNT(playerID)), 3) AS Average_Player_Salary\n       FROM salaries\n       WHERE teamID IN ('PHI', 'NYA') AND yearID &gt;= 2006\n       GROUP BY teamID\n       ORDER BY Average_Player_Salary DESC'''\n\n\nresult = pd.read_sql_query(q, con)\n\nprint(result)\n\n\n  teamID  Total_Player_Pay  Average_Player_Salary\n0    NYA      2.263671e+09            7349582.305\n1    PHI      1.399510e+09            4471278.990\n\n\nThese First Chart shown is the top 10 teams with highest total player pay over ten years and their average player salaries. The Second Chart is those same teams Average % Home Runs. When comparing these charts we see that the New York Yankies pay the most as well as have a 2% higher Home Run Average. However, the other top 8 teams are within 1.3% of each other and pay does not seem to affect their results significantly. Example is that Boston (BOS) has payed about 500 million more than the Chicogo White Socks (CHA) Yet BOS is in 5th place and CHA is in 2nd for Average Home Runs. A million dollar difference in average player salary between the two and BOS is .52% below on Home Runs. \n\n\nShow the code\nq = '''SELECT teamID, \n              ROUND(SUM(salary), 3) AS Total_Player_Pay, \n              ROUND((SUM(salary) / COUNT(playerID)), 3) AS Average_Player_Salary\n       FROM salaries\n       WHERE yearID &gt;= 2006\n       GROUP BY teamID\n       ORDER BY Average_Player_Salary DESC\n       LIMIT 10 '''\n\nq2 = '''SELECT DISTINCT teamID, \n       SUM(HR) AS Total_Home_Runs, \n       SUM(H) AS Total_Hits, \n       SUM(AB) AS Total_At_Bats, \n       (SUM(HR) * 100.0) / SUM(H) AS \"Average Home Runs %\"\nFROM teams\nWHERE teamID IN ('NYA','BOS','LAN','DET','PHI','LAA','SFN','CHN','CHA','NYN') AND yearID &gt;= 2006\nGROUP BY teamID\nORDER BY \"Average Home Runs %\" DESC'''\n\n\n\nresult = pd.read_sql_query(q, con)\n\nprint(result)\n\nresult2 = pd.read_sql_query(q2, con)\n\nprint(result2)\n\n\n  teamID  Total_Player_Pay  Average_Player_Salary\n0    NYA      2.263671e+09            7349582.305\n1    BOS      1.675458e+09            5187175.192\n2    LAN      1.598053e+09            4902001.856\n3    DET      1.457055e+09            4840714.173\n4    PHI      1.399510e+09            4471278.990\n5    LAA      1.347061e+09            4387819.583\n6    SFN      1.315003e+09            4354313.089\n7    CHN      1.243140e+09            4214034.678\n8    CHA      1.186187e+09            4133055.666\n9    NYN      1.215769e+09            3973102.520\n  teamID  Total_Home_Runs  Total_Hits  Total_At_Bats  Average Home Runs %\n0    NYA             3003       20554          77872            14.610295\n1    CHA             2544       19839          77372            12.823227\n2    PHI             2475       19607          77718            12.623043\n3    CHN             2441       19715          77357            12.381435\n4    BOS             2613       21239          78881            12.302839\n5    LAA             2382       20168          77379            11.810789\n6    NYN             2305       19593          77356            11.764406\n7    LAN             2300       19994          77067            11.503451\n8    DET             2411       20961          78357            11.502314\n9    SFN             1829       19735          77454             9.267798",
    "crumbs": [
      "DS250 Projects",
      "Project 3"
    ]
  },
  {
    "objectID": "Projects/project5.html",
    "href": "Projects/project5.html",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#elevator-pitch",
    "href": "Projects/project5.html#elevator-pitch",
    "title": "Client Report - [Insert Project Title]",
    "section": "",
    "text": "paste your elevator pitch here A SHORT (4-5 SENTENCES) PARAGRAPH THAT DESCRIBES KEY INSIGHTS TAKEN FROM METRICS IN THE PROJECT RESULTS THINK TOP OR MOST IMPORTANT RESULTS.\n\n\nRead and format project data\n# Include and execute your code here\ndf = pd.read_csv(\"https://github.com/byuidatascience/data4names/raw/master/data-raw/names_year/names_year.csv\")\n\n\nHighlight the Questions and Tasks",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-1",
    "href": "Projects/project5.html#questiontask-1",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 1",
    "text": "QUESTION|TASK 1\nCOPY PASTE QUESTION|TASK 1 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q1 chart .cell execution_count=4}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q1 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=5}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-2",
    "href": "Projects/project5.html#questiontask-2",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 2",
    "text": "QUESTION|TASK 2\nCOPY PASTE QUESTION|TASK 2 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q2 chart .cell execution_count=7}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q2 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=8}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "Projects/project5.html#questiontask-3",
    "href": "Projects/project5.html#questiontask-3",
    "title": "Client Report - [Insert Project Title]",
    "section": "QUESTION|TASK 3",
    "text": "QUESTION|TASK 3\nCOPY PASTE QUESTION|TASK 3 FROM THE PROJECT HERE\ntype your results and analysis here\n\n\nRead and format data\n# Include and execute your code here\n\n\ninclude figures in chunks and discuss your findings in the figure.\n::: {#cell-Q3 chart .cell execution_count=10}\n\nplot example\n# Include and execute your code here\nchart = px.bar(df.head(200),\n    x=\"name\", \n    y=\"AK\"\n)\nchart.show()\n\n\n                                                \nMy useless chart\n\n:::\n::: {#cell-Q3 table .cell .tbl-cap-location-top tbl-cap=‘Not much of a table’ execution_count=11}\n\ntable example\n# Include and execute your code here\nmydat = df.head(1000)\\\n    .groupby('year')\\\n    .sum()\\\n    .reset_index()\\\n    .tail(10)\\\n    .filter([\"year\", \"AK\",\"AR\"])\n\ndisplay(mydat)\n\n\n\n\n\n\n\n\n\nyear\nAK\nAR\n\n\n\n\n96\n2006\n21.0\n183.0\n\n\n97\n2007\n28.0\n153.0\n\n\n98\n2008\n36.0\n212.0\n\n\n99\n2009\n34.0\n179.0\n\n\n100\n2010\n22.0\n196.0\n\n\n101\n2011\n41.0\n148.0\n\n\n102\n2012\n28.0\n140.0\n\n\n103\n2013\n26.0\n134.0\n\n\n104\n2014\n20.0\n114.0\n\n\n105\n2015\n28.0\n121.0\n\n\n\n\n\n\n:::",
    "crumbs": [
      "DS250 Projects",
      "Project 5"
    ]
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Darren Chambers CV",
    "section": "",
    "text": "Computer Science, Cyber Security, French\n\nMy TryHackMe page | My Linkedin\n\n\n\nStudying Computer Science, Ethical Hacker, Japanese, R\n\n\nCyber Security, Computer Science, French\n\n\n\nCyber Security, Computer Hardware, drones\n\n\n\n\n2023-2024 Brigham YOung University- Idaho 2023-2024 Cisco Skills For All\n\n\n\n\n\n\n2020-2023 United States Army, North Carolina\n2019 Sales Manager,"
  },
  {
    "objectID": "resume.html#currently",
    "href": "resume.html#currently",
    "title": "Darren Chambers CV",
    "section": "",
    "text": "Studying Computer Science, Ethical Hacker, Japanese, R\n\n\nCyber Security, Computer Science, French\n\n\n\nCyber Security, Computer Hardware, drones"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Darren Chambers CV",
    "section": "",
    "text": "2023-2024 Brigham YOung University- Idaho 2023-2024 Cisco Skills For All"
  },
  {
    "objectID": "resume.html#occupation",
    "href": "resume.html#occupation",
    "title": "Darren Chambers CV",
    "section": "",
    "text": "2020-2023 United States Army, North Carolina\n2019 Sales Manager,"
  }
]